{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SUPAHOTGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfan14051/SUPA-HOT-GAN/blob/master/SUPAHOTGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "V3SmjHiBd-7T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**GAN**"
      ]
    },
    {
      "metadata": {
        "id": "IF1wud-yd9oD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4nqYeMJgaAy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "# Upload dataset. Download from Readme.md if needed.\n",
        "# TAKES A LONG TIME"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YFXFJuPOgk4v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(len(uploaded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x3QYNtl4vOeZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sample GAN from: https://skymind.ai/wiki/generative-adversarial-network-gan for use on MNIST images to show a working GAN. TURN ON TPU RUNTIME!"
      ]
    },
    {
      "metadata": {
        "id": "_OuXmx3IiOd4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, MaxPooling2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C6o5zORtgzus",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GAN():\n",
        "  def __init__(self):\n",
        "    self.img_rows = 28\n",
        "    self.img_cols = 28\n",
        "    self.channels = 1\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    optimizer = Adam(0.0002, 0.5)\n",
        "    \n",
        "    # Discriminator\n",
        "    self.discriminator = self.build_discriminator()\n",
        "    self.discriminator.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    # Generator\n",
        "    self.generator = self.build_generator()\n",
        "    self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "    z = Input(shape=(100,))\n",
        "    img = self.generator(z)\n",
        "    \n",
        "    self.discriminator.trainable = False\n",
        "    \n",
        "    valid = self.discriminator(img)\n",
        "    \n",
        "    self.combined = Model(z, valid)\n",
        "    self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "  def build_generator(self):\n",
        "    noise_shape = (100,)\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(256, input_shape = noise_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "    model.add(Reshape(self.img_shape))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    noise = Input(shape=noise_shape)\n",
        "    img = model(noise)\n",
        "    \n",
        "    return Model(noise,img)\n",
        "    \n",
        "  def build_discriminator(self):\n",
        "    img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=img_shape))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    \n",
        "    img = Input(shape=img_shape)\n",
        "    validity = model(img)\n",
        "    \n",
        "    return Model(img, validity)\n",
        "  \n",
        "  def train(self, epochs, batch_size=128, save_interval=50):\n",
        "    (X_train, _), (_,_) = mnist.load_data()\n",
        "    \n",
        "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
        "    X_train = np.expand_dims(X_train, axis=3)\n",
        "    \n",
        "    half_batch = int(batch_size / 2)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      # Discriminator\n",
        "      idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "      imgs = X_train[idx]\n",
        "      \n",
        "      noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "      \n",
        "      gen_imgs = self.generator.predict(noise)\n",
        "      \n",
        "      d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
        "      d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "      \n",
        "      # Generator\n",
        "      noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "      valid_y = np.array([1] * batch_size)\n",
        "      \n",
        "      g_loss = self.combined.train_on_batch(noise, valid_y)\n",
        "      \n",
        "      # plot progress\n",
        "      print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "      \n",
        "      # If at save interval => save generated image samples\n",
        "      if epoch % save_interval == 0:\n",
        "         self.save_imgs(epoch)\n",
        "          \n",
        "  def save_imgs(self, epoch):\n",
        "      r, c = 5, 5\n",
        "      noise = np.random.normal(0, 1, (r * c, 100))\n",
        "      gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "      # Rescale images 0 - 1\n",
        "      gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "      fig, axs = plt.subplots(r, c)\n",
        "      cnt = 0\n",
        "      for i in range(r):\n",
        "          for j in range(c):\n",
        "              axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "              axs[i,j].axis('off')\n",
        "              cnt += 1\n",
        "      fig.savefig(\"mnist_%d.png\" % epoch)\n",
        "      files.download(\"mnist_%d.png\" % epoch)\n",
        "      plt.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c0XQg9MozRLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 27860
        },
        "outputId": "b13c2b8e-939d-44ee-f6d6-67ad1019da38"
      },
      "cell_type": "code",
      "source": [
        "gan = GAN()\n",
        "gan.train(epochs=30000, batch_size=32, save_interval=200)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_23 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_154 (Dense)            (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_110 (LeakyReLU)  (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_155 (Dense)            (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_111 (LeakyReLU)  (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_156 (Dense)            (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_157 (Dense)            (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_112 (LeakyReLU)  (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_158 (Dense)            (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_113 (LeakyReLU)  (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_159 (Dense)            (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_114 (LeakyReLU)  (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_160 (Dense)            (None, 784)               803600    \n",
            "_________________________________________________________________\n",
            "reshape_22 (Reshape)         (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.931975, acc.: 40.62%] [G loss: 0.694058]\n",
            "1 [D loss: 0.522342, acc.: 65.62%] [G loss: 0.755265]\n",
            "2 [D loss: 0.318496, acc.: 90.62%] [G loss: 0.793288]\n",
            "3 [D loss: 0.297891, acc.: 81.25%] [G loss: 0.909745]\n",
            "4 [D loss: 0.270558, acc.: 96.88%] [G loss: 1.019317]\n",
            "5 [D loss: 0.222558, acc.: 96.88%] [G loss: 1.114606]\n",
            "6 [D loss: 0.197650, acc.: 96.88%] [G loss: 1.207350]\n",
            "7 [D loss: 0.185886, acc.: 100.00%] [G loss: 1.311875]\n",
            "8 [D loss: 0.203679, acc.: 93.75%] [G loss: 1.438459]\n",
            "9 [D loss: 0.151404, acc.: 100.00%] [G loss: 1.486858]\n",
            "10 [D loss: 0.164593, acc.: 96.88%] [G loss: 1.626907]\n",
            "11 [D loss: 0.137754, acc.: 100.00%] [G loss: 1.690046]\n",
            "12 [D loss: 0.108787, acc.: 100.00%] [G loss: 1.838070]\n",
            "13 [D loss: 0.123447, acc.: 100.00%] [G loss: 1.856068]\n",
            "14 [D loss: 0.107272, acc.: 100.00%] [G loss: 1.874792]\n",
            "15 [D loss: 0.114519, acc.: 100.00%] [G loss: 1.919805]\n",
            "16 [D loss: 0.068169, acc.: 100.00%] [G loss: 2.010343]\n",
            "17 [D loss: 0.092346, acc.: 100.00%] [G loss: 2.101726]\n",
            "18 [D loss: 0.066370, acc.: 100.00%] [G loss: 2.175603]\n",
            "19 [D loss: 0.083117, acc.: 100.00%] [G loss: 2.330109]\n",
            "20 [D loss: 0.072029, acc.: 100.00%] [G loss: 2.295759]\n",
            "21 [D loss: 0.066364, acc.: 100.00%] [G loss: 2.335767]\n",
            "22 [D loss: 0.073375, acc.: 100.00%] [G loss: 2.431034]\n",
            "23 [D loss: 0.053209, acc.: 100.00%] [G loss: 2.447039]\n",
            "24 [D loss: 0.062813, acc.: 100.00%] [G loss: 2.562691]\n",
            "25 [D loss: 0.051318, acc.: 100.00%] [G loss: 2.555195]\n",
            "26 [D loss: 0.055891, acc.: 100.00%] [G loss: 2.576769]\n",
            "27 [D loss: 0.049097, acc.: 100.00%] [G loss: 2.661036]\n",
            "28 [D loss: 0.056477, acc.: 100.00%] [G loss: 2.694225]\n",
            "29 [D loss: 0.037582, acc.: 100.00%] [G loss: 2.794729]\n",
            "30 [D loss: 0.051897, acc.: 100.00%] [G loss: 2.813694]\n",
            "31 [D loss: 0.048887, acc.: 100.00%] [G loss: 2.953902]\n",
            "32 [D loss: 0.030387, acc.: 100.00%] [G loss: 2.972320]\n",
            "33 [D loss: 0.032407, acc.: 100.00%] [G loss: 3.111980]\n",
            "34 [D loss: 0.034686, acc.: 100.00%] [G loss: 3.015438]\n",
            "35 [D loss: 0.036773, acc.: 100.00%] [G loss: 2.948824]\n",
            "36 [D loss: 0.053212, acc.: 100.00%] [G loss: 3.192097]\n",
            "37 [D loss: 0.027125, acc.: 100.00%] [G loss: 3.098020]\n",
            "38 [D loss: 0.036268, acc.: 100.00%] [G loss: 3.259463]\n",
            "39 [D loss: 0.029113, acc.: 100.00%] [G loss: 3.329142]\n",
            "40 [D loss: 0.041050, acc.: 100.00%] [G loss: 3.103685]\n",
            "41 [D loss: 0.033258, acc.: 100.00%] [G loss: 3.263830]\n",
            "42 [D loss: 0.022373, acc.: 100.00%] [G loss: 3.392822]\n",
            "43 [D loss: 0.026904, acc.: 100.00%] [G loss: 3.432089]\n",
            "44 [D loss: 0.022040, acc.: 100.00%] [G loss: 3.262075]\n",
            "45 [D loss: 0.030191, acc.: 100.00%] [G loss: 3.457270]\n",
            "46 [D loss: 0.031868, acc.: 100.00%] [G loss: 3.525992]\n",
            "47 [D loss: 0.029274, acc.: 100.00%] [G loss: 3.525983]\n",
            "48 [D loss: 0.014809, acc.: 100.00%] [G loss: 3.458980]\n",
            "49 [D loss: 0.029157, acc.: 100.00%] [G loss: 3.509329]\n",
            "50 [D loss: 0.024611, acc.: 100.00%] [G loss: 3.552848]\n",
            "51 [D loss: 0.023790, acc.: 100.00%] [G loss: 3.615347]\n",
            "52 [D loss: 0.021496, acc.: 100.00%] [G loss: 3.498935]\n",
            "53 [D loss: 0.030540, acc.: 100.00%] [G loss: 3.495064]\n",
            "54 [D loss: 0.034337, acc.: 100.00%] [G loss: 3.780689]\n",
            "55 [D loss: 0.016765, acc.: 100.00%] [G loss: 3.787232]\n",
            "56 [D loss: 0.026117, acc.: 100.00%] [G loss: 3.690518]\n",
            "57 [D loss: 0.023493, acc.: 100.00%] [G loss: 3.677861]\n",
            "58 [D loss: 0.013430, acc.: 100.00%] [G loss: 3.762492]\n",
            "59 [D loss: 0.019737, acc.: 100.00%] [G loss: 3.840945]\n",
            "60 [D loss: 0.026224, acc.: 100.00%] [G loss: 3.771780]\n",
            "61 [D loss: 0.019766, acc.: 100.00%] [G loss: 4.007457]\n",
            "62 [D loss: 0.014127, acc.: 100.00%] [G loss: 3.786124]\n",
            "63 [D loss: 0.013194, acc.: 100.00%] [G loss: 3.779483]\n",
            "64 [D loss: 0.015197, acc.: 100.00%] [G loss: 3.773861]\n",
            "65 [D loss: 0.029432, acc.: 100.00%] [G loss: 3.681427]\n",
            "66 [D loss: 0.016981, acc.: 100.00%] [G loss: 3.779863]\n",
            "67 [D loss: 0.021447, acc.: 100.00%] [G loss: 3.895168]\n",
            "68 [D loss: 0.030173, acc.: 100.00%] [G loss: 3.795717]\n",
            "69 [D loss: 0.018806, acc.: 100.00%] [G loss: 3.794890]\n",
            "70 [D loss: 0.024923, acc.: 100.00%] [G loss: 3.938607]\n",
            "71 [D loss: 0.024996, acc.: 100.00%] [G loss: 3.745079]\n",
            "72 [D loss: 0.018984, acc.: 100.00%] [G loss: 3.843012]\n",
            "73 [D loss: 0.012740, acc.: 100.00%] [G loss: 4.011731]\n",
            "74 [D loss: 0.017117, acc.: 100.00%] [G loss: 3.715859]\n",
            "75 [D loss: 0.019973, acc.: 100.00%] [G loss: 4.011336]\n",
            "76 [D loss: 0.028057, acc.: 100.00%] [G loss: 3.908575]\n",
            "77 [D loss: 0.034145, acc.: 100.00%] [G loss: 4.098412]\n",
            "78 [D loss: 0.014041, acc.: 100.00%] [G loss: 4.184126]\n",
            "79 [D loss: 0.019155, acc.: 100.00%] [G loss: 3.954445]\n",
            "80 [D loss: 0.020997, acc.: 100.00%] [G loss: 3.996534]\n",
            "81 [D loss: 0.030395, acc.: 100.00%] [G loss: 4.221337]\n",
            "82 [D loss: 0.027359, acc.: 100.00%] [G loss: 4.178259]\n",
            "83 [D loss: 0.009572, acc.: 100.00%] [G loss: 4.207000]\n",
            "84 [D loss: 0.030790, acc.: 100.00%] [G loss: 4.176343]\n",
            "85 [D loss: 0.014381, acc.: 100.00%] [G loss: 4.297859]\n",
            "86 [D loss: 0.020149, acc.: 100.00%] [G loss: 4.279025]\n",
            "87 [D loss: 0.028489, acc.: 100.00%] [G loss: 4.327318]\n",
            "88 [D loss: 0.027926, acc.: 100.00%] [G loss: 4.428371]\n",
            "89 [D loss: 0.023843, acc.: 100.00%] [G loss: 4.489032]\n",
            "90 [D loss: 0.012663, acc.: 100.00%] [G loss: 4.348183]\n",
            "91 [D loss: 0.015616, acc.: 100.00%] [G loss: 4.606752]\n",
            "92 [D loss: 0.014896, acc.: 100.00%] [G loss: 4.525183]\n",
            "93 [D loss: 0.015247, acc.: 100.00%] [G loss: 4.502043]\n",
            "94 [D loss: 0.026392, acc.: 100.00%] [G loss: 4.608035]\n",
            "95 [D loss: 0.019052, acc.: 100.00%] [G loss: 4.631813]\n",
            "96 [D loss: 0.020373, acc.: 100.00%] [G loss: 4.450239]\n",
            "97 [D loss: 0.018259, acc.: 100.00%] [G loss: 4.327556]\n",
            "98 [D loss: 0.017897, acc.: 100.00%] [G loss: 4.576318]\n",
            "99 [D loss: 0.010278, acc.: 100.00%] [G loss: 4.500120]\n",
            "100 [D loss: 0.021112, acc.: 100.00%] [G loss: 4.491043]\n",
            "101 [D loss: 0.014834, acc.: 100.00%] [G loss: 4.558558]\n",
            "102 [D loss: 0.027942, acc.: 100.00%] [G loss: 4.481697]\n",
            "103 [D loss: 0.017215, acc.: 100.00%] [G loss: 4.555130]\n",
            "104 [D loss: 0.015494, acc.: 100.00%] [G loss: 4.263500]\n",
            "105 [D loss: 0.023932, acc.: 100.00%] [G loss: 4.591466]\n",
            "106 [D loss: 0.017806, acc.: 100.00%] [G loss: 4.610518]\n",
            "107 [D loss: 0.037952, acc.: 100.00%] [G loss: 4.332253]\n",
            "108 [D loss: 0.029704, acc.: 100.00%] [G loss: 4.727999]\n",
            "109 [D loss: 0.025602, acc.: 100.00%] [G loss: 4.759515]\n",
            "110 [D loss: 0.026702, acc.: 100.00%] [G loss: 4.667532]\n",
            "111 [D loss: 0.058209, acc.: 100.00%] [G loss: 4.595430]\n",
            "112 [D loss: 0.050260, acc.: 100.00%] [G loss: 4.970145]\n",
            "113 [D loss: 0.031663, acc.: 100.00%] [G loss: 4.839469]\n",
            "114 [D loss: 0.130200, acc.: 93.75%] [G loss: 4.123736]\n",
            "115 [D loss: 0.045141, acc.: 96.88%] [G loss: 4.635870]\n",
            "116 [D loss: 0.021002, acc.: 100.00%] [G loss: 4.949148]\n",
            "117 [D loss: 0.032699, acc.: 100.00%] [G loss: 4.183676]\n",
            "118 [D loss: 0.041799, acc.: 100.00%] [G loss: 4.753242]\n",
            "119 [D loss: 0.050099, acc.: 96.88%] [G loss: 4.706608]\n",
            "120 [D loss: 0.381871, acc.: 87.50%] [G loss: 4.957985]\n",
            "121 [D loss: 0.162692, acc.: 96.88%] [G loss: 4.206654]\n",
            "122 [D loss: 0.018399, acc.: 100.00%] [G loss: 4.453959]\n",
            "123 [D loss: 0.156403, acc.: 93.75%] [G loss: 4.880054]\n",
            "124 [D loss: 0.187015, acc.: 93.75%] [G loss: 5.056030]\n",
            "125 [D loss: 0.885417, acc.: 62.50%] [G loss: 3.124790]\n",
            "126 [D loss: 0.343518, acc.: 87.50%] [G loss: 3.692644]\n",
            "127 [D loss: 0.243151, acc.: 90.62%] [G loss: 5.078341]\n",
            "128 [D loss: 0.169174, acc.: 93.75%] [G loss: 3.727801]\n",
            "129 [D loss: 0.036835, acc.: 100.00%] [G loss: 3.813789]\n",
            "130 [D loss: 0.189533, acc.: 90.62%] [G loss: 4.038334]\n",
            "131 [D loss: 0.064553, acc.: 100.00%] [G loss: 4.407675]\n",
            "132 [D loss: 0.102001, acc.: 96.88%] [G loss: 3.899439]\n",
            "133 [D loss: 0.226618, acc.: 87.50%] [G loss: 4.496105]\n",
            "134 [D loss: 0.316173, acc.: 81.25%] [G loss: 3.219232]\n",
            "135 [D loss: 0.146543, acc.: 90.62%] [G loss: 3.742049]\n",
            "136 [D loss: 0.029558, acc.: 100.00%] [G loss: 3.625581]\n",
            "137 [D loss: 0.113098, acc.: 96.88%] [G loss: 4.245590]\n",
            "138 [D loss: 0.145086, acc.: 100.00%] [G loss: 2.882704]\n",
            "139 [D loss: 0.208747, acc.: 90.62%] [G loss: 3.121737]\n",
            "140 [D loss: 0.206799, acc.: 90.62%] [G loss: 4.165546]\n",
            "141 [D loss: 0.585068, acc.: 71.88%] [G loss: 1.994546]\n",
            "142 [D loss: 0.584459, acc.: 75.00%] [G loss: 2.451957]\n",
            "143 [D loss: 0.362444, acc.: 75.00%] [G loss: 2.957109]\n",
            "144 [D loss: 0.165197, acc.: 87.50%] [G loss: 4.020357]\n",
            "145 [D loss: 0.051919, acc.: 100.00%] [G loss: 3.986115]\n",
            "146 [D loss: 0.429221, acc.: 81.25%] [G loss: 3.098916]\n",
            "147 [D loss: 0.178200, acc.: 90.62%] [G loss: 3.638340]\n",
            "148 [D loss: 0.263854, acc.: 90.62%] [G loss: 2.898375]\n",
            "149 [D loss: 0.120482, acc.: 93.75%] [G loss: 3.150982]\n",
            "150 [D loss: 0.227227, acc.: 87.50%] [G loss: 3.971147]\n",
            "151 [D loss: 0.334232, acc.: 84.38%] [G loss: 2.766433]\n",
            "152 [D loss: 0.120905, acc.: 96.88%] [G loss: 3.036026]\n",
            "153 [D loss: 0.151651, acc.: 93.75%] [G loss: 3.441784]\n",
            "154 [D loss: 0.299786, acc.: 84.38%] [G loss: 3.203775]\n",
            "155 [D loss: 0.119842, acc.: 93.75%] [G loss: 4.030871]\n",
            "156 [D loss: 0.132647, acc.: 96.88%] [G loss: 2.856785]\n",
            "157 [D loss: 0.122161, acc.: 93.75%] [G loss: 2.971041]\n",
            "158 [D loss: 0.098314, acc.: 100.00%] [G loss: 3.201892]\n",
            "159 [D loss: 0.147069, acc.: 96.88%] [G loss: 2.763692]\n",
            "160 [D loss: 0.111669, acc.: 96.88%] [G loss: 3.050073]\n",
            "161 [D loss: 0.297524, acc.: 84.38%] [G loss: 3.029906]\n",
            "162 [D loss: 0.206379, acc.: 93.75%] [G loss: 2.619716]\n",
            "163 [D loss: 0.114201, acc.: 96.88%] [G loss: 2.629874]\n",
            "164 [D loss: 0.372448, acc.: 81.25%] [G loss: 3.790013]\n",
            "165 [D loss: 0.972636, acc.: 50.00%] [G loss: 1.342148]\n",
            "166 [D loss: 0.509153, acc.: 78.12%] [G loss: 2.065634]\n",
            "167 [D loss: 0.182997, acc.: 87.50%] [G loss: 3.747006]\n",
            "168 [D loss: 0.090146, acc.: 100.00%] [G loss: 3.971555]\n",
            "169 [D loss: 0.239795, acc.: 90.62%] [G loss: 3.053669]\n",
            "170 [D loss: 0.183077, acc.: 90.62%] [G loss: 3.463245]\n",
            "171 [D loss: 0.393233, acc.: 84.38%] [G loss: 2.789117]\n",
            "172 [D loss: 0.208704, acc.: 93.75%] [G loss: 3.577416]\n",
            "173 [D loss: 0.233428, acc.: 90.62%] [G loss: 4.201963]\n",
            "174 [D loss: 0.979950, acc.: 50.00%] [G loss: 1.832067]\n",
            "175 [D loss: 0.452625, acc.: 75.00%] [G loss: 2.977392]\n",
            "176 [D loss: 0.145613, acc.: 93.75%] [G loss: 3.844594]\n",
            "177 [D loss: 0.080648, acc.: 100.00%] [G loss: 3.878660]\n",
            "178 [D loss: 0.491940, acc.: 75.00%] [G loss: 2.406854]\n",
            "179 [D loss: 0.115763, acc.: 100.00%] [G loss: 2.875585]\n",
            "180 [D loss: 0.149087, acc.: 93.75%] [G loss: 3.193430]\n",
            "181 [D loss: 0.311642, acc.: 87.50%] [G loss: 2.431413]\n",
            "182 [D loss: 0.168311, acc.: 93.75%] [G loss: 2.978325]\n",
            "183 [D loss: 0.380438, acc.: 84.38%] [G loss: 2.561550]\n",
            "184 [D loss: 0.366865, acc.: 84.38%] [G loss: 2.696222]\n",
            "185 [D loss: 0.318170, acc.: 93.75%] [G loss: 3.202010]\n",
            "186 [D loss: 0.295069, acc.: 87.50%] [G loss: 2.156660]\n",
            "187 [D loss: 0.210316, acc.: 93.75%] [G loss: 2.472767]\n",
            "188 [D loss: 0.197274, acc.: 90.62%] [G loss: 3.475605]\n",
            "189 [D loss: 0.823388, acc.: 56.25%] [G loss: 1.970141]\n",
            "190 [D loss: 0.150704, acc.: 96.88%] [G loss: 3.101227]\n",
            "191 [D loss: 0.510584, acc.: 75.00%] [G loss: 2.162172]\n",
            "192 [D loss: 0.141828, acc.: 93.75%] [G loss: 3.114274]\n",
            "193 [D loss: 0.218014, acc.: 93.75%] [G loss: 3.468062]\n",
            "194 [D loss: 0.485224, acc.: 81.25%] [G loss: 3.187520]\n",
            "195 [D loss: 0.347266, acc.: 78.12%] [G loss: 3.281240]\n",
            "196 [D loss: 0.303889, acc.: 90.62%] [G loss: 2.113097]\n",
            "197 [D loss: 0.281948, acc.: 84.38%] [G loss: 2.850410]\n",
            "198 [D loss: 0.435016, acc.: 84.38%] [G loss: 3.045636]\n",
            "199 [D loss: 0.402767, acc.: 81.25%] [G loss: 3.204885]\n",
            "200 [D loss: 0.299732, acc.: 84.38%] [G loss: 2.252135]\n",
            "201 [D loss: 0.237729, acc.: 90.62%] [G loss: 2.994750]\n",
            "202 [D loss: 0.293599, acc.: 90.62%] [G loss: 2.660129]\n",
            "203 [D loss: 0.515004, acc.: 62.50%] [G loss: 3.604208]\n",
            "204 [D loss: 0.984975, acc.: 46.88%] [G loss: 0.964071]\n",
            "205 [D loss: 0.553603, acc.: 78.12%] [G loss: 1.862642]\n",
            "206 [D loss: 0.183648, acc.: 93.75%] [G loss: 3.858920]\n",
            "207 [D loss: 0.778696, acc.: 53.12%] [G loss: 1.916947]\n",
            "208 [D loss: 0.157367, acc.: 93.75%] [G loss: 2.508268]\n",
            "209 [D loss: 0.593891, acc.: 62.50%] [G loss: 2.090138]\n",
            "210 [D loss: 0.297881, acc.: 87.50%] [G loss: 2.423255]\n",
            "211 [D loss: 0.518359, acc.: 68.75%] [G loss: 2.011838]\n",
            "212 [D loss: 0.285316, acc.: 93.75%] [G loss: 2.552210]\n",
            "213 [D loss: 0.792176, acc.: 50.00%] [G loss: 2.320267]\n",
            "214 [D loss: 0.516158, acc.: 68.75%] [G loss: 2.150824]\n",
            "215 [D loss: 0.361830, acc.: 87.50%] [G loss: 2.085252]\n",
            "216 [D loss: 0.488285, acc.: 78.12%] [G loss: 2.261873]\n",
            "217 [D loss: 0.367292, acc.: 87.50%] [G loss: 2.375178]\n",
            "218 [D loss: 0.371631, acc.: 84.38%] [G loss: 1.851961]\n",
            "219 [D loss: 0.315550, acc.: 87.50%] [G loss: 2.273063]\n",
            "220 [D loss: 0.398598, acc.: 78.12%] [G loss: 2.253209]\n",
            "221 [D loss: 0.278302, acc.: 96.88%] [G loss: 1.790889]\n",
            "222 [D loss: 0.463314, acc.: 78.12%] [G loss: 2.386710]\n",
            "223 [D loss: 0.346169, acc.: 84.38%] [G loss: 2.748087]\n",
            "224 [D loss: 0.540851, acc.: 71.88%] [G loss: 2.356735]\n",
            "225 [D loss: 0.320821, acc.: 87.50%] [G loss: 2.909651]\n",
            "226 [D loss: 0.555829, acc.: 75.00%] [G loss: 2.239519]\n",
            "227 [D loss: 0.403482, acc.: 75.00%] [G loss: 2.533663]\n",
            "228 [D loss: 0.522327, acc.: 78.12%] [G loss: 1.494973]\n",
            "229 [D loss: 0.381796, acc.: 81.25%] [G loss: 2.217732]\n",
            "230 [D loss: 0.373946, acc.: 81.25%] [G loss: 3.085176]\n",
            "231 [D loss: 0.649460, acc.: 71.88%] [G loss: 2.221790]\n",
            "232 [D loss: 0.316409, acc.: 87.50%] [G loss: 2.777905]\n",
            "233 [D loss: 0.811931, acc.: 59.38%] [G loss: 1.305039]\n",
            "234 [D loss: 0.449999, acc.: 68.75%] [G loss: 2.648652]\n",
            "235 [D loss: 0.490870, acc.: 75.00%] [G loss: 1.975582]\n",
            "236 [D loss: 0.484617, acc.: 75.00%] [G loss: 2.225347]\n",
            "237 [D loss: 0.576965, acc.: 59.38%] [G loss: 1.621514]\n",
            "238 [D loss: 0.353055, acc.: 81.25%] [G loss: 2.121455]\n",
            "239 [D loss: 0.876127, acc.: 46.88%] [G loss: 1.692893]\n",
            "240 [D loss: 0.473364, acc.: 81.25%] [G loss: 2.507688]\n",
            "241 [D loss: 0.870606, acc.: 59.38%] [G loss: 1.472697]\n",
            "242 [D loss: 0.508333, acc.: 71.88%] [G loss: 2.052739]\n",
            "243 [D loss: 0.702905, acc.: 56.25%] [G loss: 1.987001]\n",
            "244 [D loss: 0.458463, acc.: 78.12%] [G loss: 2.247991]\n",
            "245 [D loss: 0.826337, acc.: 43.75%] [G loss: 1.329398]\n",
            "246 [D loss: 0.492583, acc.: 75.00%] [G loss: 2.145915]\n",
            "247 [D loss: 0.917925, acc.: 43.75%] [G loss: 1.165582]\n",
            "248 [D loss: 0.506299, acc.: 68.75%] [G loss: 2.292279]\n",
            "249 [D loss: 0.473906, acc.: 84.38%] [G loss: 1.684826]\n",
            "250 [D loss: 0.352449, acc.: 87.50%] [G loss: 2.294857]\n",
            "251 [D loss: 0.641194, acc.: 68.75%] [G loss: 1.661599]\n",
            "252 [D loss: 0.476812, acc.: 71.88%] [G loss: 2.526049]\n",
            "253 [D loss: 0.774953, acc.: 43.75%] [G loss: 1.518778]\n",
            "254 [D loss: 0.476379, acc.: 71.88%] [G loss: 1.703548]\n",
            "255 [D loss: 0.565059, acc.: 75.00%] [G loss: 1.648538]\n",
            "256 [D loss: 0.493560, acc.: 75.00%] [G loss: 2.200160]\n",
            "257 [D loss: 0.780251, acc.: 50.00%] [G loss: 1.128877]\n",
            "258 [D loss: 0.749825, acc.: 37.50%] [G loss: 1.681344]\n",
            "259 [D loss: 0.563864, acc.: 65.62%] [G loss: 1.602439]\n",
            "260 [D loss: 0.679148, acc.: 56.25%] [G loss: 1.430453]\n",
            "261 [D loss: 0.557650, acc.: 68.75%] [G loss: 1.530455]\n",
            "262 [D loss: 0.583299, acc.: 65.62%] [G loss: 1.371588]\n",
            "263 [D loss: 0.837015, acc.: 43.75%] [G loss: 1.266593]\n",
            "264 [D loss: 0.568274, acc.: 62.50%] [G loss: 1.587936]\n",
            "265 [D loss: 0.664890, acc.: 56.25%] [G loss: 1.462201]\n",
            "266 [D loss: 0.538520, acc.: 68.75%] [G loss: 1.714741]\n",
            "267 [D loss: 0.696610, acc.: 50.00%] [G loss: 1.254041]\n",
            "268 [D loss: 0.621971, acc.: 53.12%] [G loss: 1.593687]\n",
            "269 [D loss: 0.781939, acc.: 40.62%] [G loss: 1.184092]\n",
            "270 [D loss: 0.622511, acc.: 53.12%] [G loss: 1.123094]\n",
            "271 [D loss: 0.718949, acc.: 53.12%] [G loss: 0.941106]\n",
            "272 [D loss: 0.564148, acc.: 59.38%] [G loss: 1.423108]\n",
            "273 [D loss: 0.629923, acc.: 53.12%] [G loss: 1.507520]\n",
            "274 [D loss: 0.519927, acc.: 68.75%] [G loss: 1.308468]\n",
            "275 [D loss: 1.022673, acc.: 18.75%] [G loss: 0.709236]\n",
            "276 [D loss: 0.583350, acc.: 53.12%] [G loss: 1.295795]\n",
            "277 [D loss: 0.739859, acc.: 43.75%] [G loss: 1.272706]\n",
            "278 [D loss: 0.728236, acc.: 50.00%] [G loss: 1.046574]\n",
            "279 [D loss: 0.595043, acc.: 71.88%] [G loss: 1.108279]\n",
            "280 [D loss: 0.569001, acc.: 71.88%] [G loss: 1.289488]\n",
            "281 [D loss: 0.606968, acc.: 65.62%] [G loss: 1.323588]\n",
            "282 [D loss: 0.775957, acc.: 46.88%] [G loss: 0.929152]\n",
            "283 [D loss: 0.650194, acc.: 59.38%] [G loss: 1.027242]\n",
            "284 [D loss: 0.675764, acc.: 53.12%] [G loss: 1.046072]\n",
            "285 [D loss: 0.651599, acc.: 62.50%] [G loss: 1.067950]\n",
            "286 [D loss: 0.703959, acc.: 53.12%] [G loss: 0.906960]\n",
            "287 [D loss: 0.812117, acc.: 43.75%] [G loss: 0.873430]\n",
            "288 [D loss: 0.830828, acc.: 34.38%] [G loss: 0.695701]\n",
            "289 [D loss: 0.684083, acc.: 56.25%] [G loss: 0.822960]\n",
            "290 [D loss: 0.757028, acc.: 43.75%] [G loss: 0.808408]\n",
            "291 [D loss: 0.781411, acc.: 43.75%] [G loss: 0.824524]\n",
            "292 [D loss: 0.768276, acc.: 40.62%] [G loss: 0.781555]\n",
            "293 [D loss: 0.699725, acc.: 50.00%] [G loss: 0.898024]\n",
            "294 [D loss: 0.716494, acc.: 46.88%] [G loss: 0.811525]\n",
            "295 [D loss: 0.740177, acc.: 46.88%] [G loss: 0.762074]\n",
            "296 [D loss: 0.734734, acc.: 40.62%] [G loss: 0.809554]\n",
            "297 [D loss: 0.639366, acc.: 53.12%] [G loss: 0.878284]\n",
            "298 [D loss: 0.596368, acc.: 71.88%] [G loss: 1.150914]\n",
            "299 [D loss: 0.751904, acc.: 46.88%] [G loss: 0.929091]\n",
            "300 [D loss: 0.600988, acc.: 68.75%] [G loss: 1.006277]\n",
            "301 [D loss: 0.724095, acc.: 46.88%] [G loss: 0.932218]\n",
            "302 [D loss: 0.795334, acc.: 34.38%] [G loss: 0.752761]\n",
            "303 [D loss: 0.652939, acc.: 50.00%] [G loss: 0.793959]\n",
            "304 [D loss: 0.703774, acc.: 46.88%] [G loss: 0.914527]\n",
            "305 [D loss: 0.651564, acc.: 50.00%] [G loss: 0.880072]\n",
            "306 [D loss: 0.728150, acc.: 50.00%] [G loss: 0.786273]\n",
            "307 [D loss: 0.690119, acc.: 50.00%] [G loss: 0.792025]\n",
            "308 [D loss: 0.688515, acc.: 46.88%] [G loss: 0.748017]\n",
            "309 [D loss: 0.673465, acc.: 59.38%] [G loss: 0.773730]\n",
            "310 [D loss: 0.757246, acc.: 43.75%] [G loss: 0.726365]\n",
            "311 [D loss: 0.705633, acc.: 46.88%] [G loss: 0.766460]\n",
            "312 [D loss: 0.653706, acc.: 56.25%] [G loss: 0.813973]\n",
            "313 [D loss: 0.618395, acc.: 68.75%] [G loss: 0.862743]\n",
            "314 [D loss: 0.796259, acc.: 28.12%] [G loss: 0.678937]\n",
            "315 [D loss: 0.683099, acc.: 46.88%] [G loss: 0.713371]\n",
            "316 [D loss: 0.650368, acc.: 53.12%] [G loss: 0.777212]\n",
            "317 [D loss: 0.682709, acc.: 50.00%] [G loss: 0.816720]\n",
            "318 [D loss: 0.703555, acc.: 43.75%] [G loss: 0.813024]\n",
            "319 [D loss: 0.691300, acc.: 43.75%] [G loss: 0.739068]\n",
            "320 [D loss: 0.704011, acc.: 50.00%] [G loss: 0.777276]\n",
            "321 [D loss: 0.733712, acc.: 31.25%] [G loss: 0.725882]\n",
            "322 [D loss: 0.639616, acc.: 56.25%] [G loss: 0.765170]\n",
            "323 [D loss: 0.683287, acc.: 59.38%] [G loss: 0.793841]\n",
            "324 [D loss: 0.668196, acc.: 50.00%] [G loss: 0.785415]\n",
            "325 [D loss: 0.611291, acc.: 68.75%] [G loss: 0.813961]\n",
            "326 [D loss: 0.705788, acc.: 56.25%] [G loss: 0.753528]\n",
            "327 [D loss: 0.626603, acc.: 59.38%] [G loss: 0.799906]\n",
            "328 [D loss: 0.651929, acc.: 68.75%] [G loss: 0.761988]\n",
            "329 [D loss: 0.661974, acc.: 59.38%] [G loss: 0.856995]\n",
            "330 [D loss: 0.739905, acc.: 46.88%] [G loss: 0.745391]\n",
            "331 [D loss: 0.663358, acc.: 56.25%] [G loss: 0.773678]\n",
            "332 [D loss: 0.733674, acc.: 43.75%] [G loss: 0.684334]\n",
            "333 [D loss: 0.649919, acc.: 50.00%] [G loss: 0.762558]\n",
            "334 [D loss: 0.653099, acc.: 53.12%] [G loss: 0.787819]\n",
            "335 [D loss: 0.649680, acc.: 62.50%] [G loss: 0.759127]\n",
            "336 [D loss: 0.690046, acc.: 43.75%] [G loss: 0.758567]\n",
            "337 [D loss: 0.666165, acc.: 56.25%] [G loss: 0.781055]\n",
            "338 [D loss: 0.661025, acc.: 59.38%] [G loss: 0.782695]\n",
            "339 [D loss: 0.702014, acc.: 56.25%] [G loss: 0.751573]\n",
            "340 [D loss: 0.693537, acc.: 50.00%] [G loss: 0.726933]\n",
            "341 [D loss: 0.660867, acc.: 56.25%] [G loss: 0.690569]\n",
            "342 [D loss: 0.671343, acc.: 53.12%] [G loss: 0.705337]\n",
            "343 [D loss: 0.625493, acc.: 62.50%] [G loss: 0.738962]\n",
            "344 [D loss: 0.713362, acc.: 43.75%] [G loss: 0.743038]\n",
            "345 [D loss: 0.714429, acc.: 46.88%] [G loss: 0.705265]\n",
            "346 [D loss: 0.693907, acc.: 40.62%] [G loss: 0.701749]\n",
            "347 [D loss: 0.680820, acc.: 53.12%] [G loss: 0.707475]\n",
            "348 [D loss: 0.691175, acc.: 50.00%] [G loss: 0.707953]\n",
            "349 [D loss: 0.713197, acc.: 40.62%] [G loss: 0.689053]\n",
            "350 [D loss: 0.696652, acc.: 43.75%] [G loss: 0.676084]\n",
            "351 [D loss: 0.701974, acc.: 53.12%] [G loss: 0.651107]\n",
            "352 [D loss: 0.693968, acc.: 53.12%] [G loss: 0.637961]\n",
            "353 [D loss: 0.712082, acc.: 43.75%] [G loss: 0.669731]\n",
            "354 [D loss: 0.688059, acc.: 43.75%] [G loss: 0.671115]\n",
            "355 [D loss: 0.679498, acc.: 50.00%] [G loss: 0.676797]\n",
            "356 [D loss: 0.692182, acc.: 46.88%] [G loss: 0.654205]\n",
            "357 [D loss: 0.652810, acc.: 50.00%] [G loss: 0.682196]\n",
            "358 [D loss: 0.678297, acc.: 56.25%] [G loss: 0.688442]\n",
            "359 [D loss: 0.712934, acc.: 40.62%] [G loss: 0.671457]\n",
            "360 [D loss: 0.725115, acc.: 43.75%] [G loss: 0.665079]\n",
            "361 [D loss: 0.677349, acc.: 50.00%] [G loss: 0.668468]\n",
            "362 [D loss: 0.719407, acc.: 50.00%] [G loss: 0.659937]\n",
            "363 [D loss: 0.680091, acc.: 43.75%] [G loss: 0.675907]\n",
            "364 [D loss: 0.703503, acc.: 50.00%] [G loss: 0.681356]\n",
            "365 [D loss: 0.663435, acc.: 50.00%] [G loss: 0.686230]\n",
            "366 [D loss: 0.731885, acc.: 40.62%] [G loss: 0.696916]\n",
            "367 [D loss: 0.660230, acc.: 53.12%] [G loss: 0.704359]\n",
            "368 [D loss: 0.676825, acc.: 53.12%] [G loss: 0.692426]\n",
            "369 [D loss: 0.697286, acc.: 53.12%] [G loss: 0.682573]\n",
            "370 [D loss: 0.686206, acc.: 46.88%] [G loss: 0.689002]\n",
            "371 [D loss: 0.701721, acc.: 37.50%] [G loss: 0.670864]\n",
            "372 [D loss: 0.672488, acc.: 46.88%] [G loss: 0.723898]\n",
            "373 [D loss: 0.657799, acc.: 59.38%] [G loss: 0.750135]\n",
            "374 [D loss: 0.677828, acc.: 59.38%] [G loss: 0.729884]\n",
            "375 [D loss: 0.703634, acc.: 46.88%] [G loss: 0.687841]\n",
            "376 [D loss: 0.670501, acc.: 50.00%] [G loss: 0.706714]\n",
            "377 [D loss: 0.669652, acc.: 56.25%] [G loss: 0.707362]\n",
            "378 [D loss: 0.677602, acc.: 62.50%] [G loss: 0.689598]\n",
            "379 [D loss: 0.650749, acc.: 56.25%] [G loss: 0.713709]\n",
            "380 [D loss: 0.660429, acc.: 59.38%] [G loss: 0.718240]\n",
            "381 [D loss: 0.697334, acc.: 43.75%] [G loss: 0.702965]\n",
            "382 [D loss: 0.674062, acc.: 46.88%] [G loss: 0.688424]\n",
            "383 [D loss: 0.681873, acc.: 46.88%] [G loss: 0.695045]\n",
            "384 [D loss: 0.709709, acc.: 40.62%] [G loss: 0.678330]\n",
            "385 [D loss: 0.668412, acc.: 50.00%] [G loss: 0.666850]\n",
            "386 [D loss: 0.684629, acc.: 50.00%] [G loss: 0.682897]\n",
            "387 [D loss: 0.687137, acc.: 46.88%] [G loss: 0.703417]\n",
            "388 [D loss: 0.702527, acc.: 43.75%] [G loss: 0.697824]\n",
            "389 [D loss: 0.664446, acc.: 53.12%] [G loss: 0.695478]\n",
            "390 [D loss: 0.638569, acc.: 65.62%] [G loss: 0.706532]\n",
            "391 [D loss: 0.688304, acc.: 59.38%] [G loss: 0.745907]\n",
            "392 [D loss: 0.654682, acc.: 62.50%] [G loss: 0.740684]\n",
            "393 [D loss: 0.690402, acc.: 50.00%] [G loss: 0.698608]\n",
            "394 [D loss: 0.655010, acc.: 62.50%] [G loss: 0.718677]\n",
            "395 [D loss: 0.640055, acc.: 68.75%] [G loss: 0.731536]\n",
            "396 [D loss: 0.690036, acc.: 59.38%] [G loss: 0.760912]\n",
            "397 [D loss: 0.734623, acc.: 34.38%] [G loss: 0.681272]\n",
            "398 [D loss: 0.679956, acc.: 56.25%] [G loss: 0.675197]\n",
            "399 [D loss: 0.657638, acc.: 53.12%] [G loss: 0.700047]\n",
            "400 [D loss: 0.702057, acc.: 53.12%] [G loss: 0.720258]\n",
            "401 [D loss: 0.688948, acc.: 56.25%] [G loss: 0.675575]\n",
            "402 [D loss: 0.684718, acc.: 50.00%] [G loss: 0.663331]\n",
            "403 [D loss: 0.678188, acc.: 50.00%] [G loss: 0.652266]\n",
            "404 [D loss: 0.705840, acc.: 46.88%] [G loss: 0.660401]\n",
            "405 [D loss: 0.680960, acc.: 53.12%] [G loss: 0.688160]\n",
            "406 [D loss: 0.670373, acc.: 50.00%] [G loss: 0.760838]\n",
            "407 [D loss: 0.627102, acc.: 59.38%] [G loss: 0.813308]\n",
            "408 [D loss: 0.710891, acc.: 50.00%] [G loss: 0.756091]\n",
            "409 [D loss: 0.687017, acc.: 46.88%] [G loss: 0.707158]\n",
            "410 [D loss: 0.682181, acc.: 46.88%] [G loss: 0.683407]\n",
            "411 [D loss: 0.663229, acc.: 46.88%] [G loss: 0.697865]\n",
            "412 [D loss: 0.676201, acc.: 46.88%] [G loss: 0.717371]\n",
            "413 [D loss: 0.740881, acc.: 34.38%] [G loss: 0.677336]\n",
            "414 [D loss: 0.643538, acc.: 59.38%] [G loss: 0.661515]\n",
            "415 [D loss: 0.678638, acc.: 46.88%] [G loss: 0.672145]\n",
            "416 [D loss: 0.695369, acc.: 46.88%] [G loss: 0.677403]\n",
            "417 [D loss: 0.652346, acc.: 65.62%] [G loss: 0.695113]\n",
            "418 [D loss: 0.718644, acc.: 43.75%] [G loss: 0.664915]\n",
            "419 [D loss: 0.640184, acc.: 59.38%] [G loss: 0.661597]\n",
            "420 [D loss: 0.663070, acc.: 50.00%] [G loss: 0.688682]\n",
            "421 [D loss: 0.636694, acc.: 59.38%] [G loss: 0.724660]\n",
            "422 [D loss: 0.653475, acc.: 50.00%] [G loss: 0.732541]\n",
            "423 [D loss: 0.668795, acc.: 50.00%] [G loss: 0.717410]\n",
            "424 [D loss: 0.683841, acc.: 59.38%] [G loss: 0.721927]\n",
            "425 [D loss: 0.692847, acc.: 46.88%] [G loss: 0.695305]\n",
            "426 [D loss: 0.677344, acc.: 53.12%] [G loss: 0.681681]\n",
            "427 [D loss: 0.656263, acc.: 56.25%] [G loss: 0.696131]\n",
            "428 [D loss: 0.691180, acc.: 43.75%] [G loss: 0.687691]\n",
            "429 [D loss: 0.644307, acc.: 50.00%] [G loss: 0.688681]\n",
            "430 [D loss: 0.653545, acc.: 53.12%] [G loss: 0.703664]\n",
            "431 [D loss: 0.645439, acc.: 50.00%] [G loss: 0.726878]\n",
            "432 [D loss: 0.641537, acc.: 43.75%] [G loss: 0.712615]\n",
            "433 [D loss: 0.633830, acc.: 56.25%] [G loss: 0.705211]\n",
            "434 [D loss: 0.648157, acc.: 46.88%] [G loss: 0.720216]\n",
            "435 [D loss: 0.631027, acc.: 56.25%] [G loss: 0.760278]\n",
            "436 [D loss: 0.642364, acc.: 56.25%] [G loss: 0.737418]\n",
            "437 [D loss: 0.642346, acc.: 50.00%] [G loss: 0.725248]\n",
            "438 [D loss: 0.622907, acc.: 50.00%] [G loss: 0.741636]\n",
            "439 [D loss: 0.595306, acc.: 65.62%] [G loss: 0.782119]\n",
            "440 [D loss: 0.651121, acc.: 71.88%] [G loss: 0.766620]\n",
            "441 [D loss: 0.652233, acc.: 62.50%] [G loss: 0.782298]\n",
            "442 [D loss: 0.633974, acc.: 65.62%] [G loss: 0.776317]\n",
            "443 [D loss: 0.662036, acc.: 50.00%] [G loss: 0.749758]\n",
            "444 [D loss: 0.580563, acc.: 68.75%] [G loss: 0.763560]\n",
            "445 [D loss: 0.606104, acc.: 62.50%] [G loss: 0.764861]\n",
            "446 [D loss: 0.592288, acc.: 71.88%] [G loss: 0.779888]\n",
            "447 [D loss: 0.599036, acc.: 65.62%] [G loss: 0.800674]\n",
            "448 [D loss: 0.648229, acc.: 59.38%] [G loss: 0.757581]\n",
            "449 [D loss: 0.597991, acc.: 65.62%] [G loss: 0.789654]\n",
            "450 [D loss: 0.618329, acc.: 71.88%] [G loss: 0.742315]\n",
            "451 [D loss: 0.656055, acc.: 53.12%] [G loss: 0.704348]\n",
            "452 [D loss: 0.641258, acc.: 46.88%] [G loss: 0.683334]\n",
            "453 [D loss: 0.598211, acc.: 56.25%] [G loss: 0.683005]\n",
            "454 [D loss: 0.606279, acc.: 59.38%] [G loss: 0.692954]\n",
            "455 [D loss: 0.650524, acc.: 56.25%] [G loss: 0.674485]\n",
            "456 [D loss: 0.664193, acc.: 53.12%] [G loss: 0.687610]\n",
            "457 [D loss: 0.654086, acc.: 56.25%] [G loss: 0.735477]\n",
            "458 [D loss: 0.589276, acc.: 62.50%] [G loss: 0.760209]\n",
            "459 [D loss: 0.616519, acc.: 62.50%] [G loss: 0.742631]\n",
            "460 [D loss: 0.601881, acc.: 65.62%] [G loss: 0.743822]\n",
            "461 [D loss: 0.645606, acc.: 68.75%] [G loss: 0.737487]\n",
            "462 [D loss: 0.671912, acc.: 59.38%] [G loss: 0.749094]\n",
            "463 [D loss: 0.608496, acc.: 53.12%] [G loss: 0.782901]\n",
            "464 [D loss: 0.635289, acc.: 56.25%] [G loss: 0.749687]\n",
            "465 [D loss: 0.629392, acc.: 65.62%] [G loss: 0.749669]\n",
            "466 [D loss: 0.621591, acc.: 59.38%] [G loss: 0.747725]\n",
            "467 [D loss: 0.646009, acc.: 56.25%] [G loss: 0.740289]\n",
            "468 [D loss: 0.649396, acc.: 59.38%] [G loss: 0.762673]\n",
            "469 [D loss: 0.624801, acc.: 56.25%] [G loss: 0.737881]\n",
            "470 [D loss: 0.633553, acc.: 53.12%] [G loss: 0.725078]\n",
            "471 [D loss: 0.616391, acc.: 59.38%] [G loss: 0.720993]\n",
            "472 [D loss: 0.678591, acc.: 53.12%] [G loss: 0.767159]\n",
            "473 [D loss: 0.614675, acc.: 59.38%] [G loss: 0.767408]\n",
            "474 [D loss: 0.635844, acc.: 56.25%] [G loss: 0.780597]\n",
            "475 [D loss: 0.640563, acc.: 56.25%] [G loss: 0.743588]\n",
            "476 [D loss: 0.643705, acc.: 50.00%] [G loss: 0.747352]\n",
            "477 [D loss: 0.673965, acc.: 50.00%] [G loss: 0.734797]\n",
            "478 [D loss: 0.667948, acc.: 56.25%] [G loss: 0.745297]\n",
            "479 [D loss: 0.646314, acc.: 50.00%] [G loss: 0.733840]\n",
            "480 [D loss: 0.629126, acc.: 62.50%] [G loss: 0.698804]\n",
            "481 [D loss: 0.651878, acc.: 65.62%] [G loss: 0.744153]\n",
            "482 [D loss: 0.663641, acc.: 50.00%] [G loss: 0.728786]\n",
            "483 [D loss: 0.637259, acc.: 62.50%] [G loss: 0.719247]\n",
            "484 [D loss: 0.620635, acc.: 65.62%] [G loss: 0.753356]\n",
            "485 [D loss: 0.719632, acc.: 56.25%] [G loss: 0.764107]\n",
            "486 [D loss: 0.631050, acc.: 68.75%] [G loss: 0.811559]\n",
            "487 [D loss: 0.712528, acc.: 43.75%] [G loss: 0.781707]\n",
            "488 [D loss: 0.690413, acc.: 46.88%] [G loss: 0.709315]\n",
            "489 [D loss: 0.676417, acc.: 50.00%] [G loss: 0.705235]\n",
            "490 [D loss: 0.648831, acc.: 46.88%] [G loss: 0.714598]\n",
            "491 [D loss: 0.610610, acc.: 56.25%] [G loss: 0.701312]\n",
            "492 [D loss: 0.687263, acc.: 50.00%] [G loss: 0.690879]\n",
            "493 [D loss: 0.646348, acc.: 53.12%] [G loss: 0.723006]\n",
            "494 [D loss: 0.657136, acc.: 56.25%] [G loss: 0.752741]\n",
            "495 [D loss: 0.649146, acc.: 56.25%] [G loss: 0.776437]\n",
            "496 [D loss: 0.600165, acc.: 68.75%] [G loss: 0.786897]\n",
            "497 [D loss: 0.655975, acc.: 53.12%] [G loss: 0.764052]\n",
            "498 [D loss: 0.690173, acc.: 43.75%] [G loss: 0.763675]\n",
            "499 [D loss: 0.669882, acc.: 53.12%] [G loss: 0.754455]\n",
            "500 [D loss: 0.680452, acc.: 50.00%] [G loss: 0.768101]\n",
            "501 [D loss: 0.662736, acc.: 56.25%] [G loss: 0.777593]\n",
            "502 [D loss: 0.698566, acc.: 40.62%] [G loss: 0.708557]\n",
            "503 [D loss: 0.670276, acc.: 43.75%] [G loss: 0.716133]\n",
            "504 [D loss: 0.696314, acc.: 43.75%] [G loss: 0.729460]\n",
            "505 [D loss: 0.677209, acc.: 50.00%] [G loss: 0.719490]\n",
            "506 [D loss: 0.692091, acc.: 46.88%] [G loss: 0.710126]\n",
            "507 [D loss: 0.660421, acc.: 46.88%] [G loss: 0.715569]\n",
            "508 [D loss: 0.642034, acc.: 50.00%] [G loss: 0.747820]\n",
            "509 [D loss: 0.633258, acc.: 65.62%] [G loss: 0.746783]\n",
            "510 [D loss: 0.643282, acc.: 62.50%] [G loss: 0.749655]\n",
            "511 [D loss: 0.662991, acc.: 50.00%] [G loss: 0.744034]\n",
            "512 [D loss: 0.667360, acc.: 65.62%] [G loss: 0.753954]\n",
            "513 [D loss: 0.655852, acc.: 62.50%] [G loss: 0.743531]\n",
            "514 [D loss: 0.714405, acc.: 40.62%] [G loss: 0.686247]\n",
            "515 [D loss: 0.678522, acc.: 50.00%] [G loss: 0.674007]\n",
            "516 [D loss: 0.672960, acc.: 53.12%] [G loss: 0.669641]\n",
            "517 [D loss: 0.643012, acc.: 56.25%] [G loss: 0.697913]\n",
            "518 [D loss: 0.646104, acc.: 53.12%] [G loss: 0.731939]\n",
            "519 [D loss: 0.665372, acc.: 59.38%] [G loss: 0.740673]\n",
            "520 [D loss: 0.668105, acc.: 50.00%] [G loss: 0.739078]\n",
            "521 [D loss: 0.632949, acc.: 75.00%] [G loss: 0.727050]\n",
            "522 [D loss: 0.697749, acc.: 56.25%] [G loss: 0.722836]\n",
            "523 [D loss: 0.645521, acc.: 71.88%] [G loss: 0.730127]\n",
            "524 [D loss: 0.619374, acc.: 68.75%] [G loss: 0.738841]\n",
            "525 [D loss: 0.594320, acc.: 68.75%] [G loss: 0.729699]\n",
            "526 [D loss: 0.615276, acc.: 62.50%] [G loss: 0.743140]\n",
            "527 [D loss: 0.663926, acc.: 59.38%] [G loss: 0.749963]\n",
            "528 [D loss: 0.610132, acc.: 71.88%] [G loss: 0.768285]\n",
            "529 [D loss: 0.607673, acc.: 75.00%] [G loss: 0.721571]\n",
            "530 [D loss: 0.660453, acc.: 62.50%] [G loss: 0.715852]\n",
            "531 [D loss: 0.629386, acc.: 62.50%] [G loss: 0.728864]\n",
            "532 [D loss: 0.658241, acc.: 59.38%] [G loss: 0.734674]\n",
            "533 [D loss: 0.661742, acc.: 59.38%] [G loss: 0.752506]\n",
            "534 [D loss: 0.656501, acc.: 50.00%] [G loss: 0.743988]\n",
            "535 [D loss: 0.655365, acc.: 46.88%] [G loss: 0.734410]\n",
            "536 [D loss: 0.610010, acc.: 56.25%] [G loss: 0.750811]\n",
            "537 [D loss: 0.685020, acc.: 37.50%] [G loss: 0.748973]\n",
            "538 [D loss: 0.665232, acc.: 50.00%] [G loss: 0.713469]\n",
            "539 [D loss: 0.688493, acc.: 37.50%] [G loss: 0.700394]\n",
            "540 [D loss: 0.636758, acc.: 50.00%] [G loss: 0.718352]\n",
            "541 [D loss: 0.650505, acc.: 53.12%] [G loss: 0.740813]\n",
            "542 [D loss: 0.689880, acc.: 53.12%] [G loss: 0.745349]\n",
            "543 [D loss: 0.622028, acc.: 59.38%] [G loss: 0.766042]\n",
            "544 [D loss: 0.706486, acc.: 40.62%] [G loss: 0.715071]\n",
            "545 [D loss: 0.671636, acc.: 56.25%] [G loss: 0.710328]\n",
            "546 [D loss: 0.679896, acc.: 56.25%] [G loss: 0.724939]\n",
            "547 [D loss: 0.676953, acc.: 53.12%] [G loss: 0.673451]\n",
            "548 [D loss: 0.645758, acc.: 43.75%] [G loss: 0.697574]\n",
            "549 [D loss: 0.624054, acc.: 59.38%] [G loss: 0.724496]\n",
            "550 [D loss: 0.596507, acc.: 59.38%] [G loss: 0.790102]\n",
            "551 [D loss: 0.644126, acc.: 56.25%] [G loss: 0.833925]\n",
            "552 [D loss: 0.673415, acc.: 50.00%] [G loss: 0.796236]\n",
            "553 [D loss: 0.656368, acc.: 53.12%] [G loss: 0.744008]\n",
            "554 [D loss: 0.630569, acc.: 53.12%] [G loss: 0.731143]\n",
            "555 [D loss: 0.641390, acc.: 53.12%] [G loss: 0.712245]\n",
            "556 [D loss: 0.675537, acc.: 50.00%] [G loss: 0.684355]\n",
            "557 [D loss: 0.652380, acc.: 50.00%] [G loss: 0.697830]\n",
            "558 [D loss: 0.638107, acc.: 56.25%] [G loss: 0.726809]\n",
            "559 [D loss: 0.644754, acc.: 59.38%] [G loss: 0.739563]\n",
            "560 [D loss: 0.595251, acc.: 68.75%] [G loss: 0.780374]\n",
            "561 [D loss: 0.701056, acc.: 56.25%] [G loss: 0.755909]\n",
            "562 [D loss: 0.655135, acc.: 56.25%] [G loss: 0.788102]\n",
            "563 [D loss: 0.669878, acc.: 56.25%] [G loss: 0.776932]\n",
            "564 [D loss: 0.636429, acc.: 53.12%] [G loss: 0.759101]\n",
            "565 [D loss: 0.641498, acc.: 59.38%] [G loss: 0.775474]\n",
            "566 [D loss: 0.639796, acc.: 68.75%] [G loss: 0.763801]\n",
            "567 [D loss: 0.617598, acc.: 75.00%] [G loss: 0.776131]\n",
            "568 [D loss: 0.643341, acc.: 65.62%] [G loss: 0.771782]\n",
            "569 [D loss: 0.631436, acc.: 65.62%] [G loss: 0.807426]\n",
            "570 [D loss: 0.661014, acc.: 53.12%] [G loss: 0.817012]\n",
            "571 [D loss: 0.645537, acc.: 56.25%] [G loss: 0.787675]\n",
            "572 [D loss: 0.653138, acc.: 56.25%] [G loss: 0.762557]\n",
            "573 [D loss: 0.655900, acc.: 50.00%] [G loss: 0.744425]\n",
            "574 [D loss: 0.657306, acc.: 53.12%] [G loss: 0.724749]\n",
            "575 [D loss: 0.626040, acc.: 59.38%] [G loss: 0.721339]\n",
            "576 [D loss: 0.621467, acc.: 65.62%] [G loss: 0.735628]\n",
            "577 [D loss: 0.661453, acc.: 56.25%] [G loss: 0.745228]\n",
            "578 [D loss: 0.664204, acc.: 62.50%] [G loss: 0.761400]\n",
            "579 [D loss: 0.646880, acc.: 53.12%] [G loss: 0.778216]\n",
            "580 [D loss: 0.632202, acc.: 71.88%] [G loss: 0.770629]\n",
            "581 [D loss: 0.665235, acc.: 50.00%] [G loss: 0.747933]\n",
            "582 [D loss: 0.668900, acc.: 62.50%] [G loss: 0.717077]\n",
            "583 [D loss: 0.643072, acc.: 75.00%] [G loss: 0.713844]\n",
            "584 [D loss: 0.661407, acc.: 56.25%] [G loss: 0.722791]\n",
            "585 [D loss: 0.685231, acc.: 46.88%] [G loss: 0.749162]\n",
            "586 [D loss: 0.624378, acc.: 62.50%] [G loss: 0.762412]\n",
            "587 [D loss: 0.652566, acc.: 68.75%] [G loss: 0.786902]\n",
            "588 [D loss: 0.628151, acc.: 59.38%] [G loss: 0.794344]\n",
            "589 [D loss: 0.646712, acc.: 59.38%] [G loss: 0.788782]\n",
            "590 [D loss: 0.620272, acc.: 75.00%] [G loss: 0.814091]\n",
            "591 [D loss: 0.629151, acc.: 71.88%] [G loss: 0.837405]\n",
            "592 [D loss: 0.603380, acc.: 75.00%] [G loss: 0.846445]\n",
            "593 [D loss: 0.668229, acc.: 65.62%] [G loss: 0.781841]\n",
            "594 [D loss: 0.630117, acc.: 65.62%] [G loss: 0.740439]\n",
            "595 [D loss: 0.670816, acc.: 56.25%] [G loss: 0.730142]\n",
            "596 [D loss: 0.633964, acc.: 59.38%] [G loss: 0.767936]\n",
            "597 [D loss: 0.646571, acc.: 59.38%] [G loss: 0.762772]\n",
            "598 [D loss: 0.677802, acc.: 56.25%] [G loss: 0.743824]\n",
            "599 [D loss: 0.644778, acc.: 59.38%] [G loss: 0.727174]\n",
            "600 [D loss: 0.663418, acc.: 56.25%] [G loss: 0.705545]\n",
            "601 [D loss: 0.654903, acc.: 62.50%] [G loss: 0.721494]\n",
            "602 [D loss: 0.671527, acc.: 62.50%] [G loss: 0.712881]\n",
            "603 [D loss: 0.661654, acc.: 62.50%] [G loss: 0.708061]\n",
            "604 [D loss: 0.677529, acc.: 59.38%] [G loss: 0.701991]\n",
            "605 [D loss: 0.651097, acc.: 62.50%] [G loss: 0.739141]\n",
            "606 [D loss: 0.648314, acc.: 62.50%] [G loss: 0.771561]\n",
            "607 [D loss: 0.683549, acc.: 56.25%] [G loss: 0.721284]\n",
            "608 [D loss: 0.738124, acc.: 28.12%] [G loss: 0.691655]\n",
            "609 [D loss: 0.722911, acc.: 46.88%] [G loss: 0.701250]\n",
            "610 [D loss: 0.654844, acc.: 53.12%] [G loss: 0.787602]\n",
            "611 [D loss: 0.603335, acc.: 78.12%] [G loss: 0.857382]\n",
            "612 [D loss: 0.686202, acc.: 37.50%] [G loss: 0.825116]\n",
            "613 [D loss: 0.613150, acc.: 68.75%] [G loss: 0.813569]\n",
            "614 [D loss: 0.573348, acc.: 68.75%] [G loss: 0.820688]\n",
            "615 [D loss: 0.591781, acc.: 75.00%] [G loss: 0.823594]\n",
            "616 [D loss: 0.592922, acc.: 71.88%] [G loss: 0.796315]\n",
            "617 [D loss: 0.639891, acc.: 53.12%] [G loss: 0.774999]\n",
            "618 [D loss: 0.686069, acc.: 50.00%] [G loss: 0.777317]\n",
            "619 [D loss: 0.629264, acc.: 46.88%] [G loss: 0.806573]\n",
            "620 [D loss: 0.639081, acc.: 59.38%] [G loss: 0.768878]\n",
            "621 [D loss: 0.602120, acc.: 56.25%] [G loss: 0.785685]\n",
            "622 [D loss: 0.637076, acc.: 65.62%] [G loss: 0.794470]\n",
            "623 [D loss: 0.658998, acc.: 53.12%] [G loss: 0.736338]\n",
            "624 [D loss: 0.646308, acc.: 56.25%] [G loss: 0.720842]\n",
            "625 [D loss: 0.653455, acc.: 46.88%] [G loss: 0.723181]\n",
            "626 [D loss: 0.638654, acc.: 56.25%] [G loss: 0.743587]\n",
            "627 [D loss: 0.630716, acc.: 71.88%] [G loss: 0.753816]\n",
            "628 [D loss: 0.612303, acc.: 78.12%] [G loss: 0.765089]\n",
            "629 [D loss: 0.635533, acc.: 71.88%] [G loss: 0.780533]\n",
            "630 [D loss: 0.654467, acc.: 56.25%] [G loss: 0.823977]\n",
            "631 [D loss: 0.601124, acc.: 65.62%] [G loss: 0.867730]\n",
            "632 [D loss: 0.614256, acc.: 81.25%] [G loss: 0.826116]\n",
            "633 [D loss: 0.651712, acc.: 62.50%] [G loss: 0.816209]\n",
            "634 [D loss: 0.644257, acc.: 59.38%] [G loss: 0.846126]\n",
            "635 [D loss: 0.647790, acc.: 62.50%] [G loss: 0.785403]\n",
            "636 [D loss: 0.655336, acc.: 53.12%] [G loss: 0.781135]\n",
            "637 [D loss: 0.596857, acc.: 62.50%] [G loss: 0.760356]\n",
            "638 [D loss: 0.680483, acc.: 53.12%] [G loss: 0.741909]\n",
            "639 [D loss: 0.639961, acc.: 62.50%] [G loss: 0.733756]\n",
            "640 [D loss: 0.637483, acc.: 59.38%] [G loss: 0.694413]\n",
            "641 [D loss: 0.661284, acc.: 59.38%] [G loss: 0.703937]\n",
            "642 [D loss: 0.657664, acc.: 50.00%] [G loss: 0.716384]\n",
            "643 [D loss: 0.646688, acc.: 53.12%] [G loss: 0.738750]\n",
            "644 [D loss: 0.620192, acc.: 65.62%] [G loss: 0.804738]\n",
            "645 [D loss: 0.636208, acc.: 62.50%] [G loss: 0.799526]\n",
            "646 [D loss: 0.574375, acc.: 78.12%] [G loss: 0.841908]\n",
            "647 [D loss: 0.665301, acc.: 50.00%] [G loss: 0.811832]\n",
            "648 [D loss: 0.596807, acc.: 81.25%] [G loss: 0.868762]\n",
            "649 [D loss: 0.626868, acc.: 65.62%] [G loss: 0.833881]\n",
            "650 [D loss: 0.623684, acc.: 62.50%] [G loss: 0.792861]\n",
            "651 [D loss: 0.571138, acc.: 68.75%] [G loss: 0.827419]\n",
            "652 [D loss: 0.647679, acc.: 65.62%] [G loss: 0.813637]\n",
            "653 [D loss: 0.654667, acc.: 53.12%] [G loss: 0.770408]\n",
            "654 [D loss: 0.617346, acc.: 68.75%] [G loss: 0.785896]\n",
            "655 [D loss: 0.595607, acc.: 75.00%] [G loss: 0.782862]\n",
            "656 [D loss: 0.625871, acc.: 62.50%] [G loss: 0.804790]\n",
            "657 [D loss: 0.676543, acc.: 62.50%] [G loss: 0.800478]\n",
            "658 [D loss: 0.600502, acc.: 71.88%] [G loss: 0.777943]\n",
            "659 [D loss: 0.651030, acc.: 53.12%] [G loss: 0.768691]\n",
            "660 [D loss: 0.610367, acc.: 65.62%] [G loss: 0.796260]\n",
            "661 [D loss: 0.583460, acc.: 81.25%] [G loss: 0.775966]\n",
            "662 [D loss: 0.632500, acc.: 71.88%] [G loss: 0.803900]\n",
            "663 [D loss: 0.649928, acc.: 71.88%] [G loss: 0.769673]\n",
            "664 [D loss: 0.651442, acc.: 62.50%] [G loss: 0.779589]\n",
            "665 [D loss: 0.630459, acc.: 68.75%] [G loss: 0.771906]\n",
            "666 [D loss: 0.668490, acc.: 59.38%] [G loss: 0.759219]\n",
            "667 [D loss: 0.559929, acc.: 81.25%] [G loss: 0.831055]\n",
            "668 [D loss: 0.632699, acc.: 65.62%] [G loss: 0.824739]\n",
            "669 [D loss: 0.613806, acc.: 78.12%] [G loss: 0.832377]\n",
            "670 [D loss: 0.624687, acc.: 65.62%] [G loss: 0.813222]\n",
            "671 [D loss: 0.596779, acc.: 68.75%] [G loss: 0.849610]\n",
            "672 [D loss: 0.669492, acc.: 53.12%] [G loss: 0.822080]\n",
            "673 [D loss: 0.671172, acc.: 56.25%] [G loss: 0.830077]\n",
            "674 [D loss: 0.664925, acc.: 62.50%] [G loss: 0.813219]\n",
            "675 [D loss: 0.681951, acc.: 53.12%] [G loss: 0.739432]\n",
            "676 [D loss: 0.677635, acc.: 56.25%] [G loss: 0.751843]\n",
            "677 [D loss: 0.656790, acc.: 59.38%] [G loss: 0.777869]\n",
            "678 [D loss: 0.662040, acc.: 62.50%] [G loss: 0.900210]\n",
            "679 [D loss: 0.625514, acc.: 71.88%] [G loss: 0.882692]\n",
            "680 [D loss: 0.660259, acc.: 65.62%] [G loss: 0.798322]\n",
            "681 [D loss: 0.684400, acc.: 46.88%] [G loss: 0.845651]\n",
            "682 [D loss: 0.685652, acc.: 62.50%] [G loss: 0.856467]\n",
            "683 [D loss: 0.644831, acc.: 65.62%] [G loss: 0.873934]\n",
            "684 [D loss: 0.574398, acc.: 81.25%] [G loss: 0.914114]\n",
            "685 [D loss: 0.589500, acc.: 81.25%] [G loss: 0.871302]\n",
            "686 [D loss: 0.682087, acc.: 50.00%] [G loss: 0.802597]\n",
            "687 [D loss: 0.678445, acc.: 56.25%] [G loss: 0.800369]\n",
            "688 [D loss: 0.714088, acc.: 43.75%] [G loss: 0.796222]\n",
            "689 [D loss: 0.596512, acc.: 78.12%] [G loss: 0.834795]\n",
            "690 [D loss: 0.618854, acc.: 78.12%] [G loss: 0.876035]\n",
            "691 [D loss: 0.632574, acc.: 62.50%] [G loss: 0.855933]\n",
            "692 [D loss: 0.626922, acc.: 68.75%] [G loss: 0.851580]\n",
            "693 [D loss: 0.618928, acc.: 68.75%] [G loss: 0.823504]\n",
            "694 [D loss: 0.651648, acc.: 53.12%] [G loss: 0.790305]\n",
            "695 [D loss: 0.614701, acc.: 59.38%] [G loss: 0.826544]\n",
            "696 [D loss: 0.607974, acc.: 59.38%] [G loss: 0.822249]\n",
            "697 [D loss: 0.554672, acc.: 78.12%] [G loss: 0.826958]\n",
            "698 [D loss: 0.614302, acc.: 68.75%] [G loss: 0.809272]\n",
            "699 [D loss: 0.595939, acc.: 78.12%] [G loss: 0.831464]\n",
            "700 [D loss: 0.612110, acc.: 62.50%] [G loss: 0.829397]\n",
            "701 [D loss: 0.643946, acc.: 68.75%] [G loss: 0.830093]\n",
            "702 [D loss: 0.608742, acc.: 75.00%] [G loss: 0.798697]\n",
            "703 [D loss: 0.625155, acc.: 59.38%] [G loss: 0.814167]\n",
            "704 [D loss: 0.659110, acc.: 59.38%] [G loss: 0.804222]\n",
            "705 [D loss: 0.632167, acc.: 68.75%] [G loss: 0.816862]\n",
            "706 [D loss: 0.592616, acc.: 78.12%] [G loss: 0.837019]\n",
            "707 [D loss: 0.665047, acc.: 56.25%] [G loss: 0.844227]\n",
            "708 [D loss: 0.592407, acc.: 84.38%] [G loss: 0.870522]\n",
            "709 [D loss: 0.598072, acc.: 87.50%] [G loss: 0.847182]\n",
            "710 [D loss: 0.631225, acc.: 65.62%] [G loss: 0.826373]\n",
            "711 [D loss: 0.594785, acc.: 68.75%] [G loss: 0.833311]\n",
            "712 [D loss: 0.626867, acc.: 59.38%] [G loss: 0.814501]\n",
            "713 [D loss: 0.608130, acc.: 75.00%] [G loss: 0.757354]\n",
            "714 [D loss: 0.657617, acc.: 59.38%] [G loss: 0.750999]\n",
            "715 [D loss: 0.619640, acc.: 59.38%] [G loss: 0.789589]\n",
            "716 [D loss: 0.647605, acc.: 53.12%] [G loss: 0.787467]\n",
            "717 [D loss: 0.580602, acc.: 87.50%] [G loss: 0.758541]\n",
            "718 [D loss: 0.705973, acc.: 68.75%] [G loss: 0.770936]\n",
            "719 [D loss: 0.626468, acc.: 68.75%] [G loss: 0.787179]\n",
            "720 [D loss: 0.619747, acc.: 68.75%] [G loss: 0.794617]\n",
            "721 [D loss: 0.678304, acc.: 68.75%] [G loss: 0.832833]\n",
            "722 [D loss: 0.579317, acc.: 93.75%] [G loss: 0.854922]\n",
            "723 [D loss: 0.612710, acc.: 68.75%] [G loss: 0.845939]\n",
            "724 [D loss: 0.628158, acc.: 62.50%] [G loss: 0.815313]\n",
            "725 [D loss: 0.614928, acc.: 68.75%] [G loss: 0.910199]\n",
            "726 [D loss: 0.644688, acc.: 62.50%] [G loss: 0.929980]\n",
            "727 [D loss: 0.604047, acc.: 71.88%] [G loss: 0.892399]\n",
            "728 [D loss: 0.677184, acc.: 50.00%] [G loss: 0.807100]\n",
            "729 [D loss: 0.652860, acc.: 62.50%] [G loss: 0.847966]\n",
            "730 [D loss: 0.603262, acc.: 71.88%] [G loss: 0.860717]\n",
            "731 [D loss: 0.578125, acc.: 75.00%] [G loss: 0.841555]\n",
            "732 [D loss: 0.638650, acc.: 81.25%] [G loss: 0.834237]\n",
            "733 [D loss: 0.615258, acc.: 65.62%] [G loss: 0.889299]\n",
            "734 [D loss: 0.630894, acc.: 62.50%] [G loss: 0.839607]\n",
            "735 [D loss: 0.666745, acc.: 56.25%] [G loss: 0.790097]\n",
            "736 [D loss: 0.646957, acc.: 62.50%] [G loss: 0.827474]\n",
            "737 [D loss: 0.609878, acc.: 75.00%] [G loss: 0.832946]\n",
            "738 [D loss: 0.622188, acc.: 68.75%] [G loss: 0.792974]\n",
            "739 [D loss: 0.567503, acc.: 75.00%] [G loss: 0.792433]\n",
            "740 [D loss: 0.629206, acc.: 68.75%] [G loss: 0.779318]\n",
            "741 [D loss: 0.636620, acc.: 65.62%] [G loss: 0.804912]\n",
            "742 [D loss: 0.660498, acc.: 46.88%] [G loss: 0.826635]\n",
            "743 [D loss: 0.626989, acc.: 68.75%] [G loss: 0.806886]\n",
            "744 [D loss: 0.576204, acc.: 71.88%] [G loss: 0.846998]\n",
            "745 [D loss: 0.600273, acc.: 78.12%] [G loss: 0.866565]\n",
            "746 [D loss: 0.650366, acc.: 53.12%] [G loss: 0.783819]\n",
            "747 [D loss: 0.672019, acc.: 56.25%] [G loss: 0.731714]\n",
            "748 [D loss: 0.595341, acc.: 62.50%] [G loss: 0.769326]\n",
            "749 [D loss: 0.670227, acc.: 62.50%] [G loss: 0.768642]\n",
            "750 [D loss: 0.605895, acc.: 84.38%] [G loss: 0.810015]\n",
            "751 [D loss: 0.645948, acc.: 65.62%] [G loss: 0.775142]\n",
            "752 [D loss: 0.620081, acc.: 68.75%] [G loss: 0.777702]\n",
            "753 [D loss: 0.568065, acc.: 78.12%] [G loss: 0.777183]\n",
            "754 [D loss: 0.602702, acc.: 68.75%] [G loss: 0.770016]\n",
            "755 [D loss: 0.629987, acc.: 71.88%] [G loss: 0.796391]\n",
            "756 [D loss: 0.611090, acc.: 62.50%] [G loss: 0.939554]\n",
            "757 [D loss: 0.565187, acc.: 84.38%] [G loss: 0.950572]\n",
            "758 [D loss: 0.617867, acc.: 71.88%] [G loss: 0.823154]\n",
            "759 [D loss: 0.621416, acc.: 65.62%] [G loss: 0.787111]\n",
            "760 [D loss: 0.613352, acc.: 81.25%] [G loss: 0.795052]\n",
            "761 [D loss: 0.641139, acc.: 71.88%] [G loss: 0.783684]\n",
            "762 [D loss: 0.575646, acc.: 71.88%] [G loss: 0.870520]\n",
            "763 [D loss: 0.642042, acc.: 75.00%] [G loss: 0.885383]\n",
            "764 [D loss: 0.624714, acc.: 68.75%] [G loss: 0.882025]\n",
            "765 [D loss: 0.636134, acc.: 68.75%] [G loss: 0.811761]\n",
            "766 [D loss: 0.581643, acc.: 68.75%] [G loss: 0.821434]\n",
            "767 [D loss: 0.595643, acc.: 75.00%] [G loss: 0.866596]\n",
            "768 [D loss: 0.662478, acc.: 50.00%] [G loss: 0.863677]\n",
            "769 [D loss: 0.591976, acc.: 71.88%] [G loss: 0.876864]\n",
            "770 [D loss: 0.551250, acc.: 84.38%] [G loss: 0.942848]\n",
            "771 [D loss: 0.619340, acc.: 68.75%] [G loss: 0.880622]\n",
            "772 [D loss: 0.604142, acc.: 71.88%] [G loss: 0.876081]\n",
            "773 [D loss: 0.566520, acc.: 75.00%] [G loss: 0.797358]\n",
            "774 [D loss: 0.607549, acc.: 65.62%] [G loss: 0.760138]\n",
            "775 [D loss: 0.590340, acc.: 71.88%] [G loss: 0.792767]\n",
            "776 [D loss: 0.559096, acc.: 78.12%] [G loss: 0.822793]\n",
            "777 [D loss: 0.689615, acc.: 46.88%] [G loss: 0.852694]\n",
            "778 [D loss: 0.652485, acc.: 53.12%] [G loss: 0.803130]\n",
            "779 [D loss: 0.621086, acc.: 56.25%] [G loss: 0.855757]\n",
            "780 [D loss: 0.598962, acc.: 56.25%] [G loss: 0.880971]\n",
            "781 [D loss: 0.620503, acc.: 68.75%] [G loss: 0.874687]\n",
            "782 [D loss: 0.612978, acc.: 78.12%] [G loss: 0.796459]\n",
            "783 [D loss: 0.631088, acc.: 62.50%] [G loss: 0.785913]\n",
            "784 [D loss: 0.643872, acc.: 56.25%] [G loss: 0.812690]\n",
            "785 [D loss: 0.585650, acc.: 81.25%] [G loss: 0.798862]\n",
            "786 [D loss: 0.648597, acc.: 62.50%] [G loss: 0.784976]\n",
            "787 [D loss: 0.653747, acc.: 68.75%] [G loss: 0.823139]\n",
            "788 [D loss: 0.617691, acc.: 71.88%] [G loss: 0.810892]\n",
            "789 [D loss: 0.696742, acc.: 53.12%] [G loss: 0.836345]\n",
            "790 [D loss: 0.566335, acc.: 78.12%] [G loss: 0.868077]\n",
            "791 [D loss: 0.641825, acc.: 53.12%] [G loss: 0.805473]\n",
            "792 [D loss: 0.601503, acc.: 71.88%] [G loss: 0.799879]\n",
            "793 [D loss: 0.590564, acc.: 65.62%] [G loss: 0.863817]\n",
            "794 [D loss: 0.624841, acc.: 68.75%] [G loss: 0.909625]\n",
            "795 [D loss: 0.618729, acc.: 75.00%] [G loss: 0.864211]\n",
            "796 [D loss: 0.619925, acc.: 75.00%] [G loss: 0.813696]\n",
            "797 [D loss: 0.648691, acc.: 75.00%] [G loss: 0.792721]\n",
            "798 [D loss: 0.626511, acc.: 65.62%] [G loss: 0.771932]\n",
            "799 [D loss: 0.622931, acc.: 68.75%] [G loss: 0.842884]\n",
            "800 [D loss: 0.592277, acc.: 81.25%] [G loss: 0.854113]\n",
            "801 [D loss: 0.648514, acc.: 46.88%] [G loss: 0.828003]\n",
            "802 [D loss: 0.640116, acc.: 65.62%] [G loss: 0.790724]\n",
            "803 [D loss: 0.626116, acc.: 53.12%] [G loss: 0.824932]\n",
            "804 [D loss: 0.636598, acc.: 59.38%] [G loss: 0.828858]\n",
            "805 [D loss: 0.583135, acc.: 81.25%] [G loss: 0.828632]\n",
            "806 [D loss: 0.683630, acc.: 50.00%] [G loss: 0.810468]\n",
            "807 [D loss: 0.642839, acc.: 59.38%] [G loss: 0.787285]\n",
            "808 [D loss: 0.625104, acc.: 71.88%] [G loss: 0.800772]\n",
            "809 [D loss: 0.643916, acc.: 59.38%] [G loss: 0.801958]\n",
            "810 [D loss: 0.604229, acc.: 68.75%] [G loss: 0.793855]\n",
            "811 [D loss: 0.659933, acc.: 50.00%] [G loss: 0.836229]\n",
            "812 [D loss: 0.582261, acc.: 81.25%] [G loss: 0.822489]\n",
            "813 [D loss: 0.642087, acc.: 59.38%] [G loss: 0.751930]\n",
            "814 [D loss: 0.635505, acc.: 59.38%] [G loss: 0.806312]\n",
            "815 [D loss: 0.669356, acc.: 59.38%] [G loss: 0.839723]\n",
            "816 [D loss: 0.635818, acc.: 62.50%] [G loss: 0.815274]\n",
            "817 [D loss: 0.606559, acc.: 65.62%] [G loss: 0.811682]\n",
            "818 [D loss: 0.631224, acc.: 68.75%] [G loss: 0.811723]\n",
            "819 [D loss: 0.628854, acc.: 53.12%] [G loss: 0.834595]\n",
            "820 [D loss: 0.624667, acc.: 65.62%] [G loss: 0.842144]\n",
            "821 [D loss: 0.634860, acc.: 65.62%] [G loss: 0.823035]\n",
            "822 [D loss: 0.609086, acc.: 56.25%] [G loss: 0.779487]\n",
            "823 [D loss: 0.649041, acc.: 59.38%] [G loss: 0.755984]\n",
            "824 [D loss: 0.561869, acc.: 75.00%] [G loss: 0.804662]\n",
            "825 [D loss: 0.619637, acc.: 68.75%] [G loss: 0.845565]\n",
            "826 [D loss: 0.654130, acc.: 62.50%] [G loss: 0.881127]\n",
            "827 [D loss: 0.690695, acc.: 40.62%] [G loss: 0.806982]\n",
            "828 [D loss: 0.689154, acc.: 59.38%] [G loss: 0.799764]\n",
            "829 [D loss: 0.600407, acc.: 65.62%] [G loss: 0.872303]\n",
            "830 [D loss: 0.640412, acc.: 68.75%] [G loss: 0.929135]\n",
            "831 [D loss: 0.651032, acc.: 71.88%] [G loss: 0.909675]\n",
            "832 [D loss: 0.680408, acc.: 59.38%] [G loss: 0.830829]\n",
            "833 [D loss: 0.674588, acc.: 46.88%] [G loss: 0.788499]\n",
            "834 [D loss: 0.663489, acc.: 50.00%] [G loss: 0.789240]\n",
            "835 [D loss: 0.669285, acc.: 68.75%] [G loss: 0.784611]\n",
            "836 [D loss: 0.595657, acc.: 65.62%] [G loss: 0.804670]\n",
            "837 [D loss: 0.619683, acc.: 71.88%] [G loss: 0.748865]\n",
            "838 [D loss: 0.733555, acc.: 56.25%] [G loss: 0.746158]\n",
            "839 [D loss: 0.700885, acc.: 62.50%] [G loss: 0.823677]\n",
            "840 [D loss: 0.617894, acc.: 62.50%] [G loss: 0.892324]\n",
            "841 [D loss: 0.655744, acc.: 59.38%] [G loss: 0.861513]\n",
            "842 [D loss: 0.687615, acc.: 53.12%] [G loss: 0.785213]\n",
            "843 [D loss: 0.612042, acc.: 59.38%] [G loss: 0.848224]\n",
            "844 [D loss: 0.640349, acc.: 59.38%] [G loss: 0.857002]\n",
            "845 [D loss: 0.634258, acc.: 59.38%] [G loss: 0.884553]\n",
            "846 [D loss: 0.685912, acc.: 46.88%] [G loss: 0.793380]\n",
            "847 [D loss: 0.613088, acc.: 68.75%] [G loss: 0.772877]\n",
            "848 [D loss: 0.652793, acc.: 50.00%] [G loss: 0.798921]\n",
            "849 [D loss: 0.707987, acc.: 50.00%] [G loss: 0.764328]\n",
            "850 [D loss: 0.618760, acc.: 65.62%] [G loss: 0.805928]\n",
            "851 [D loss: 0.618951, acc.: 68.75%] [G loss: 0.818560]\n",
            "852 [D loss: 0.631477, acc.: 68.75%] [G loss: 0.791058]\n",
            "853 [D loss: 0.654046, acc.: 68.75%] [G loss: 0.766621]\n",
            "854 [D loss: 0.616271, acc.: 62.50%] [G loss: 0.768827]\n",
            "855 [D loss: 0.660017, acc.: 62.50%] [G loss: 0.773227]\n",
            "856 [D loss: 0.646615, acc.: 65.62%] [G loss: 0.768423]\n",
            "857 [D loss: 0.680552, acc.: 53.12%] [G loss: 0.722497]\n",
            "858 [D loss: 0.681732, acc.: 62.50%] [G loss: 0.753960]\n",
            "859 [D loss: 0.631183, acc.: 68.75%] [G loss: 0.756003]\n",
            "860 [D loss: 0.602305, acc.: 84.38%] [G loss: 0.728025]\n",
            "861 [D loss: 0.608574, acc.: 62.50%] [G loss: 0.735545]\n",
            "862 [D loss: 0.632070, acc.: 62.50%] [G loss: 0.756255]\n",
            "863 [D loss: 0.658601, acc.: 65.62%] [G loss: 0.721117]\n",
            "864 [D loss: 0.639311, acc.: 62.50%] [G loss: 0.745473]\n",
            "865 [D loss: 0.602100, acc.: 81.25%] [G loss: 0.718868]\n",
            "866 [D loss: 0.653873, acc.: 68.75%] [G loss: 0.794287]\n",
            "867 [D loss: 0.652226, acc.: 59.38%] [G loss: 0.810249]\n",
            "868 [D loss: 0.603760, acc.: 71.88%] [G loss: 0.788924]\n",
            "869 [D loss: 0.625105, acc.: 62.50%] [G loss: 0.769522]\n",
            "870 [D loss: 0.629385, acc.: 62.50%] [G loss: 0.803480]\n",
            "871 [D loss: 0.679081, acc.: 56.25%] [G loss: 0.807544]\n",
            "872 [D loss: 0.578132, acc.: 81.25%] [G loss: 0.806175]\n",
            "873 [D loss: 0.657343, acc.: 56.25%] [G loss: 0.789003]\n",
            "874 [D loss: 0.671941, acc.: 59.38%] [G loss: 0.759080]\n",
            "875 [D loss: 0.636145, acc.: 53.12%] [G loss: 0.797530]\n",
            "876 [D loss: 0.628161, acc.: 65.62%] [G loss: 0.831498]\n",
            "877 [D loss: 0.719694, acc.: 37.50%] [G loss: 0.754273]\n",
            "878 [D loss: 0.722128, acc.: 37.50%] [G loss: 0.762557]\n",
            "879 [D loss: 0.642193, acc.: 62.50%] [G loss: 0.776176]\n",
            "880 [D loss: 0.692002, acc.: 53.12%] [G loss: 0.766827]\n",
            "881 [D loss: 0.666208, acc.: 53.12%] [G loss: 0.757060]\n",
            "882 [D loss: 0.674885, acc.: 59.38%] [G loss: 0.777822]\n",
            "883 [D loss: 0.691040, acc.: 50.00%] [G loss: 0.789283]\n",
            "884 [D loss: 0.692019, acc.: 43.75%] [G loss: 0.788233]\n",
            "885 [D loss: 0.678115, acc.: 59.38%] [G loss: 0.785329]\n",
            "886 [D loss: 0.662892, acc.: 56.25%] [G loss: 0.772711]\n",
            "887 [D loss: 0.680119, acc.: 62.50%] [G loss: 0.743004]\n",
            "888 [D loss: 0.629081, acc.: 68.75%] [G loss: 0.773997]\n",
            "889 [D loss: 0.688344, acc.: 68.75%] [G loss: 0.754909]\n",
            "890 [D loss: 0.673183, acc.: 59.38%] [G loss: 0.748535]\n",
            "891 [D loss: 0.661333, acc.: 56.25%] [G loss: 0.745255]\n",
            "892 [D loss: 0.703713, acc.: 53.12%] [G loss: 0.742919]\n",
            "893 [D loss: 0.662858, acc.: 50.00%] [G loss: 0.737203]\n",
            "894 [D loss: 0.630033, acc.: 65.62%] [G loss: 0.777133]\n",
            "895 [D loss: 0.698031, acc.: 46.88%] [G loss: 0.764451]\n",
            "896 [D loss: 0.645442, acc.: 62.50%] [G loss: 0.788964]\n",
            "897 [D loss: 0.611246, acc.: 75.00%] [G loss: 0.783476]\n",
            "898 [D loss: 0.637908, acc.: 56.25%] [G loss: 0.784263]\n",
            "899 [D loss: 0.648018, acc.: 59.38%] [G loss: 0.779066]\n",
            "900 [D loss: 0.626292, acc.: 65.62%] [G loss: 0.797267]\n",
            "901 [D loss: 0.646821, acc.: 53.12%] [G loss: 0.798025]\n",
            "902 [D loss: 0.560256, acc.: 84.38%] [G loss: 0.761099]\n",
            "903 [D loss: 0.630719, acc.: 71.88%] [G loss: 0.750489]\n",
            "904 [D loss: 0.584287, acc.: 84.38%] [G loss: 0.809593]\n",
            "905 [D loss: 0.623567, acc.: 68.75%] [G loss: 0.803937]\n",
            "906 [D loss: 0.638711, acc.: 62.50%] [G loss: 0.779823]\n",
            "907 [D loss: 0.577511, acc.: 78.12%] [G loss: 0.793401]\n",
            "908 [D loss: 0.629878, acc.: 65.62%] [G loss: 0.851624]\n",
            "909 [D loss: 0.636061, acc.: 56.25%] [G loss: 0.801956]\n",
            "910 [D loss: 0.709407, acc.: 46.88%] [G loss: 0.783773]\n",
            "911 [D loss: 0.611944, acc.: 71.88%] [G loss: 0.822232]\n",
            "912 [D loss: 0.637023, acc.: 62.50%] [G loss: 0.823003]\n",
            "913 [D loss: 0.639763, acc.: 68.75%] [G loss: 0.815985]\n",
            "914 [D loss: 0.619096, acc.: 78.12%] [G loss: 0.821019]\n",
            "915 [D loss: 0.666938, acc.: 68.75%] [G loss: 0.779440]\n",
            "916 [D loss: 0.631897, acc.: 59.38%] [G loss: 0.756091]\n",
            "917 [D loss: 0.614201, acc.: 75.00%] [G loss: 0.765633]\n",
            "918 [D loss: 0.596307, acc.: 71.88%] [G loss: 0.789053]\n",
            "919 [D loss: 0.641218, acc.: 59.38%] [G loss: 0.769329]\n",
            "920 [D loss: 0.617573, acc.: 78.12%] [G loss: 0.743397]\n",
            "921 [D loss: 0.651416, acc.: 59.38%] [G loss: 0.744542]\n",
            "922 [D loss: 0.645123, acc.: 59.38%] [G loss: 0.837585]\n",
            "923 [D loss: 0.597526, acc.: 75.00%] [G loss: 0.880644]\n",
            "924 [D loss: 0.617678, acc.: 75.00%] [G loss: 0.809038]\n",
            "925 [D loss: 0.612791, acc.: 78.12%] [G loss: 0.865196]\n",
            "926 [D loss: 0.647539, acc.: 62.50%] [G loss: 0.835998]\n",
            "927 [D loss: 0.664236, acc.: 53.12%] [G loss: 0.765533]\n",
            "928 [D loss: 0.649441, acc.: 53.12%] [G loss: 0.726091]\n",
            "929 [D loss: 0.617919, acc.: 59.38%] [G loss: 0.753597]\n",
            "930 [D loss: 0.649862, acc.: 53.12%] [G loss: 0.775858]\n",
            "931 [D loss: 0.658883, acc.: 56.25%] [G loss: 0.808489]\n",
            "932 [D loss: 0.638483, acc.: 75.00%] [G loss: 0.822623]\n",
            "933 [D loss: 0.650403, acc.: 65.62%] [G loss: 0.807375]\n",
            "934 [D loss: 0.579421, acc.: 78.12%] [G loss: 0.838211]\n",
            "935 [D loss: 0.682941, acc.: 59.38%] [G loss: 0.828447]\n",
            "936 [D loss: 0.742923, acc.: 43.75%] [G loss: 0.780316]\n",
            "937 [D loss: 0.586411, acc.: 75.00%] [G loss: 0.794071]\n",
            "938 [D loss: 0.600075, acc.: 68.75%] [G loss: 0.830088]\n",
            "939 [D loss: 0.680875, acc.: 56.25%] [G loss: 0.807269]\n",
            "940 [D loss: 0.667699, acc.: 50.00%] [G loss: 0.790035]\n",
            "941 [D loss: 0.655882, acc.: 62.50%] [G loss: 0.818408]\n",
            "942 [D loss: 0.705441, acc.: 50.00%] [G loss: 0.773422]\n",
            "943 [D loss: 0.637861, acc.: 65.62%] [G loss: 0.787056]\n",
            "944 [D loss: 0.631680, acc.: 68.75%] [G loss: 0.819110]\n",
            "945 [D loss: 0.625239, acc.: 68.75%] [G loss: 0.787295]\n",
            "946 [D loss: 0.611457, acc.: 65.62%] [G loss: 0.764230]\n",
            "947 [D loss: 0.663067, acc.: 59.38%] [G loss: 0.753200]\n",
            "948 [D loss: 0.622253, acc.: 68.75%] [G loss: 0.776016]\n",
            "949 [D loss: 0.666460, acc.: 56.25%] [G loss: 0.821159]\n",
            "950 [D loss: 0.627626, acc.: 71.88%] [G loss: 0.828825]\n",
            "951 [D loss: 0.679805, acc.: 46.88%] [G loss: 0.803963]\n",
            "952 [D loss: 0.637057, acc.: 68.75%] [G loss: 0.780497]\n",
            "953 [D loss: 0.633198, acc.: 62.50%] [G loss: 0.735974]\n",
            "954 [D loss: 0.658291, acc.: 65.62%] [G loss: 0.786302]\n",
            "955 [D loss: 0.627493, acc.: 62.50%] [G loss: 0.806808]\n",
            "956 [D loss: 0.661013, acc.: 62.50%] [G loss: 0.800643]\n",
            "957 [D loss: 0.618903, acc.: 68.75%] [G loss: 0.820886]\n",
            "958 [D loss: 0.627162, acc.: 75.00%] [G loss: 0.773391]\n",
            "959 [D loss: 0.633004, acc.: 65.62%] [G loss: 0.744303]\n",
            "960 [D loss: 0.620232, acc.: 68.75%] [G loss: 0.774867]\n",
            "961 [D loss: 0.687874, acc.: 37.50%] [G loss: 0.759501]\n",
            "962 [D loss: 0.612919, acc.: 65.62%] [G loss: 0.747604]\n",
            "963 [D loss: 0.685211, acc.: 37.50%] [G loss: 0.745931]\n",
            "964 [D loss: 0.672645, acc.: 59.38%] [G loss: 0.771370]\n",
            "965 [D loss: 0.662136, acc.: 59.38%] [G loss: 0.725667]\n",
            "966 [D loss: 0.609575, acc.: 81.25%] [G loss: 0.727871]\n",
            "967 [D loss: 0.705464, acc.: 53.12%] [G loss: 0.733896]\n",
            "968 [D loss: 0.597027, acc.: 78.12%] [G loss: 0.708314]\n",
            "969 [D loss: 0.671999, acc.: 56.25%] [G loss: 0.709416]\n",
            "970 [D loss: 0.643001, acc.: 71.88%] [G loss: 0.750971]\n",
            "971 [D loss: 0.649346, acc.: 59.38%] [G loss: 0.731295]\n",
            "972 [D loss: 0.651079, acc.: 65.62%] [G loss: 0.778157]\n",
            "973 [D loss: 0.589636, acc.: 84.38%] [G loss: 0.712571]\n",
            "974 [D loss: 0.693974, acc.: 71.88%] [G loss: 0.774039]\n",
            "975 [D loss: 0.617737, acc.: 71.88%] [G loss: 0.844339]\n",
            "976 [D loss: 0.616238, acc.: 68.75%] [G loss: 0.815395]\n",
            "977 [D loss: 0.644109, acc.: 65.62%] [G loss: 0.856296]\n",
            "978 [D loss: 0.644830, acc.: 71.88%] [G loss: 0.845806]\n",
            "979 [D loss: 0.664947, acc.: 62.50%] [G loss: 0.861344]\n",
            "980 [D loss: 0.614979, acc.: 65.62%] [G loss: 0.850184]\n",
            "981 [D loss: 0.585573, acc.: 75.00%] [G loss: 0.831467]\n",
            "982 [D loss: 0.587292, acc.: 81.25%] [G loss: 0.896569]\n",
            "983 [D loss: 0.626036, acc.: 78.12%] [G loss: 0.908238]\n",
            "984 [D loss: 0.564434, acc.: 78.12%] [G loss: 0.832251]\n",
            "985 [D loss: 0.658848, acc.: 59.38%] [G loss: 0.787952]\n",
            "986 [D loss: 0.690233, acc.: 50.00%] [G loss: 0.764596]\n",
            "987 [D loss: 0.614221, acc.: 71.88%] [G loss: 0.785812]\n",
            "988 [D loss: 0.681993, acc.: 59.38%] [G loss: 0.817878]\n",
            "989 [D loss: 0.604220, acc.: 71.88%] [G loss: 0.823759]\n",
            "990 [D loss: 0.621004, acc.: 75.00%] [G loss: 0.815757]\n",
            "991 [D loss: 0.579563, acc.: 75.00%] [G loss: 0.806130]\n",
            "992 [D loss: 0.719397, acc.: 50.00%] [G loss: 0.821786]\n",
            "993 [D loss: 0.632683, acc.: 56.25%] [G loss: 0.805282]\n",
            "994 [D loss: 0.652386, acc.: 62.50%] [G loss: 0.786650]\n",
            "995 [D loss: 0.630327, acc.: 65.62%] [G loss: 0.825377]\n",
            "996 [D loss: 0.618644, acc.: 68.75%] [G loss: 0.800354]\n",
            "997 [D loss: 0.634627, acc.: 65.62%] [G loss: 0.771721]\n",
            "998 [D loss: 0.621745, acc.: 71.88%] [G loss: 0.755735]\n",
            "999 [D loss: 0.618739, acc.: 65.62%] [G loss: 0.772800]\n",
            "1000 [D loss: 0.583413, acc.: 81.25%] [G loss: 0.822689]\n",
            "1001 [D loss: 0.614707, acc.: 68.75%] [G loss: 0.844969]\n",
            "1002 [D loss: 0.594438, acc.: 65.62%] [G loss: 0.829322]\n",
            "1003 [D loss: 0.668572, acc.: 62.50%] [G loss: 0.791326]\n",
            "1004 [D loss: 0.604458, acc.: 71.88%] [G loss: 0.807058]\n",
            "1005 [D loss: 0.666815, acc.: 59.38%] [G loss: 0.815744]\n",
            "1006 [D loss: 0.677938, acc.: 56.25%] [G loss: 0.812263]\n",
            "1007 [D loss: 0.598512, acc.: 81.25%] [G loss: 0.772252]\n",
            "1008 [D loss: 0.621704, acc.: 65.62%] [G loss: 0.816059]\n",
            "1009 [D loss: 0.604169, acc.: 81.25%] [G loss: 0.819455]\n",
            "1010 [D loss: 0.614416, acc.: 78.12%] [G loss: 0.822292]\n",
            "1011 [D loss: 0.593462, acc.: 81.25%] [G loss: 0.870451]\n",
            "1012 [D loss: 0.647866, acc.: 56.25%] [G loss: 0.843335]\n",
            "1013 [D loss: 0.584380, acc.: 71.88%] [G loss: 0.830148]\n",
            "1014 [D loss: 0.600418, acc.: 65.62%] [G loss: 0.825710]\n",
            "1015 [D loss: 0.586627, acc.: 65.62%] [G loss: 0.812499]\n",
            "1016 [D loss: 0.654507, acc.: 59.38%] [G loss: 0.775686]\n",
            "1017 [D loss: 0.634687, acc.: 68.75%] [G loss: 0.828048]\n",
            "1018 [D loss: 0.601604, acc.: 68.75%] [G loss: 0.808769]\n",
            "1019 [D loss: 0.644746, acc.: 68.75%] [G loss: 0.854445]\n",
            "1020 [D loss: 0.678237, acc.: 56.25%] [G loss: 0.839723]\n",
            "1021 [D loss: 0.685936, acc.: 43.75%] [G loss: 0.807817]\n",
            "1022 [D loss: 0.659069, acc.: 50.00%] [G loss: 0.791959]\n",
            "1023 [D loss: 0.643163, acc.: 56.25%] [G loss: 0.851392]\n",
            "1024 [D loss: 0.660527, acc.: 59.38%] [G loss: 0.865653]\n",
            "1025 [D loss: 0.612333, acc.: 75.00%] [G loss: 0.940340]\n",
            "1026 [D loss: 0.650268, acc.: 62.50%] [G loss: 0.973238]\n",
            "1027 [D loss: 0.581315, acc.: 81.25%] [G loss: 0.909082]\n",
            "1028 [D loss: 0.688229, acc.: 53.12%] [G loss: 0.814553]\n",
            "1029 [D loss: 0.655364, acc.: 53.12%] [G loss: 0.858207]\n",
            "1030 [D loss: 0.621797, acc.: 68.75%] [G loss: 0.856595]\n",
            "1031 [D loss: 0.668730, acc.: 46.88%] [G loss: 0.840155]\n",
            "1032 [D loss: 0.628133, acc.: 62.50%] [G loss: 0.809921]\n",
            "1033 [D loss: 0.628918, acc.: 62.50%] [G loss: 0.806326]\n",
            "1034 [D loss: 0.656530, acc.: 62.50%] [G loss: 0.804002]\n",
            "1035 [D loss: 0.593930, acc.: 78.12%] [G loss: 0.783077]\n",
            "1036 [D loss: 0.654330, acc.: 68.75%] [G loss: 0.865247]\n",
            "1037 [D loss: 0.628474, acc.: 68.75%] [G loss: 0.811404]\n",
            "1038 [D loss: 0.624429, acc.: 68.75%] [G loss: 0.824417]\n",
            "1039 [D loss: 0.648096, acc.: 62.50%] [G loss: 0.799356]\n",
            "1040 [D loss: 0.606304, acc.: 68.75%] [G loss: 0.795284]\n",
            "1041 [D loss: 0.599928, acc.: 71.88%] [G loss: 0.837036]\n",
            "1042 [D loss: 0.565244, acc.: 84.38%] [G loss: 0.851715]\n",
            "1043 [D loss: 0.654891, acc.: 59.38%] [G loss: 0.814931]\n",
            "1044 [D loss: 0.630756, acc.: 65.62%] [G loss: 0.818033]\n",
            "1045 [D loss: 0.614654, acc.: 68.75%] [G loss: 0.805623]\n",
            "1046 [D loss: 0.615282, acc.: 68.75%] [G loss: 0.817549]\n",
            "1047 [D loss: 0.635422, acc.: 65.62%] [G loss: 0.810827]\n",
            "1048 [D loss: 0.681943, acc.: 62.50%] [G loss: 0.838804]\n",
            "1049 [D loss: 0.567864, acc.: 75.00%] [G loss: 0.848459]\n",
            "1050 [D loss: 0.587305, acc.: 71.88%] [G loss: 0.897497]\n",
            "1051 [D loss: 0.674461, acc.: 71.88%] [G loss: 0.900677]\n",
            "1052 [D loss: 0.663590, acc.: 59.38%] [G loss: 0.886297]\n",
            "1053 [D loss: 0.619539, acc.: 71.88%] [G loss: 0.799602]\n",
            "1054 [D loss: 0.608666, acc.: 68.75%] [G loss: 0.886601]\n",
            "1055 [D loss: 0.577539, acc.: 81.25%] [G loss: 0.819006]\n",
            "1056 [D loss: 0.588085, acc.: 78.12%] [G loss: 0.790676]\n",
            "1057 [D loss: 0.621925, acc.: 68.75%] [G loss: 0.784063]\n",
            "1058 [D loss: 0.595194, acc.: 75.00%] [G loss: 0.797284]\n",
            "1059 [D loss: 0.672764, acc.: 62.50%] [G loss: 0.870061]\n",
            "1060 [D loss: 0.623013, acc.: 68.75%] [G loss: 0.890164]\n",
            "1061 [D loss: 0.625768, acc.: 62.50%] [G loss: 0.837337]\n",
            "1062 [D loss: 0.649512, acc.: 59.38%] [G loss: 0.843971]\n",
            "1063 [D loss: 0.628190, acc.: 56.25%] [G loss: 0.832794]\n",
            "1064 [D loss: 0.634351, acc.: 53.12%] [G loss: 0.880998]\n",
            "1065 [D loss: 0.639108, acc.: 56.25%] [G loss: 0.850165]\n",
            "1066 [D loss: 0.616856, acc.: 81.25%] [G loss: 0.848019]\n",
            "1067 [D loss: 0.565844, acc.: 87.50%] [G loss: 0.869038]\n",
            "1068 [D loss: 0.625751, acc.: 62.50%] [G loss: 0.840274]\n",
            "1069 [D loss: 0.678721, acc.: 53.12%] [G loss: 0.862121]\n",
            "1070 [D loss: 0.657497, acc.: 56.25%] [G loss: 0.922886]\n",
            "1071 [D loss: 0.565807, acc.: 75.00%] [G loss: 0.914564]\n",
            "1072 [D loss: 0.631029, acc.: 65.62%] [G loss: 0.862540]\n",
            "1073 [D loss: 0.638595, acc.: 59.38%] [G loss: 0.793811]\n",
            "1074 [D loss: 0.668305, acc.: 46.88%] [G loss: 0.752135]\n",
            "1075 [D loss: 0.628596, acc.: 65.62%] [G loss: 0.817984]\n",
            "1076 [D loss: 0.605073, acc.: 59.38%] [G loss: 0.794531]\n",
            "1077 [D loss: 0.685923, acc.: 53.12%] [G loss: 0.775163]\n",
            "1078 [D loss: 0.588399, acc.: 68.75%] [G loss: 0.826581]\n",
            "1079 [D loss: 0.544320, acc.: 84.38%] [G loss: 0.758152]\n",
            "1080 [D loss: 0.607890, acc.: 68.75%] [G loss: 0.790143]\n",
            "1081 [D loss: 0.679903, acc.: 53.12%] [G loss: 0.771754]\n",
            "1082 [D loss: 0.649631, acc.: 56.25%] [G loss: 0.838455]\n",
            "1083 [D loss: 0.739681, acc.: 46.88%] [G loss: 0.913700]\n",
            "1084 [D loss: 0.620784, acc.: 65.62%] [G loss: 0.968340]\n",
            "1085 [D loss: 0.650513, acc.: 56.25%] [G loss: 0.925190]\n",
            "1086 [D loss: 0.619411, acc.: 68.75%] [G loss: 0.916569]\n",
            "1087 [D loss: 0.624925, acc.: 65.62%] [G loss: 0.855001]\n",
            "1088 [D loss: 0.612492, acc.: 65.62%] [G loss: 0.940520]\n",
            "1089 [D loss: 0.645875, acc.: 62.50%] [G loss: 0.858517]\n",
            "1090 [D loss: 0.660152, acc.: 59.38%] [G loss: 0.869946]\n",
            "1091 [D loss: 0.623177, acc.: 71.88%] [G loss: 0.995954]\n",
            "1092 [D loss: 0.665738, acc.: 59.38%] [G loss: 0.933603]\n",
            "1093 [D loss: 0.611610, acc.: 71.88%] [G loss: 0.925321]\n",
            "1094 [D loss: 0.633288, acc.: 59.38%] [G loss: 0.883601]\n",
            "1095 [D loss: 0.684716, acc.: 65.62%] [G loss: 0.829989]\n",
            "1096 [D loss: 0.616831, acc.: 56.25%] [G loss: 0.871218]\n",
            "1097 [D loss: 0.577752, acc.: 71.88%] [G loss: 0.908115]\n",
            "1098 [D loss: 0.619712, acc.: 59.38%] [G loss: 0.916352]\n",
            "1099 [D loss: 0.639634, acc.: 56.25%] [G loss: 0.906579]\n",
            "1100 [D loss: 0.620453, acc.: 62.50%] [G loss: 0.887326]\n",
            "1101 [D loss: 0.714779, acc.: 50.00%] [G loss: 0.857376]\n",
            "1102 [D loss: 0.626191, acc.: 68.75%] [G loss: 0.880028]\n",
            "1103 [D loss: 0.616472, acc.: 65.62%] [G loss: 0.865241]\n",
            "1104 [D loss: 0.595968, acc.: 81.25%] [G loss: 0.851828]\n",
            "1105 [D loss: 0.661357, acc.: 65.62%] [G loss: 0.830793]\n",
            "1106 [D loss: 0.657198, acc.: 59.38%] [G loss: 0.825632]\n",
            "1107 [D loss: 0.608874, acc.: 68.75%] [G loss: 0.849262]\n",
            "1108 [D loss: 0.570626, acc.: 81.25%] [G loss: 0.828266]\n",
            "1109 [D loss: 0.649963, acc.: 68.75%] [G loss: 0.820779]\n",
            "1110 [D loss: 0.608989, acc.: 68.75%] [G loss: 0.807273]\n",
            "1111 [D loss: 0.626329, acc.: 53.12%] [G loss: 0.745321]\n",
            "1112 [D loss: 0.649920, acc.: 59.38%] [G loss: 0.769629]\n",
            "1113 [D loss: 0.642479, acc.: 62.50%] [G loss: 0.803781]\n",
            "1114 [D loss: 0.643402, acc.: 50.00%] [G loss: 0.840181]\n",
            "1115 [D loss: 0.563120, acc.: 78.12%] [G loss: 0.777898]\n",
            "1116 [D loss: 0.722421, acc.: 46.88%] [G loss: 0.807798]\n",
            "1117 [D loss: 0.714763, acc.: 43.75%] [G loss: 0.891276]\n",
            "1118 [D loss: 0.569389, acc.: 81.25%] [G loss: 0.932721]\n",
            "1119 [D loss: 0.638696, acc.: 71.88%] [G loss: 0.934300]\n",
            "1120 [D loss: 0.695585, acc.: 56.25%] [G loss: 0.808211]\n",
            "1121 [D loss: 0.556658, acc.: 78.12%] [G loss: 0.840892]\n",
            "1122 [D loss: 0.610703, acc.: 75.00%] [G loss: 0.802969]\n",
            "1123 [D loss: 0.608690, acc.: 65.62%] [G loss: 0.871922]\n",
            "1124 [D loss: 0.612557, acc.: 71.88%] [G loss: 0.862529]\n",
            "1125 [D loss: 0.593453, acc.: 75.00%] [G loss: 0.853288]\n",
            "1126 [D loss: 0.651500, acc.: 62.50%] [G loss: 0.851028]\n",
            "1127 [D loss: 0.616815, acc.: 62.50%] [G loss: 0.855276]\n",
            "1128 [D loss: 0.653863, acc.: 53.12%] [G loss: 0.811020]\n",
            "1129 [D loss: 0.579163, acc.: 71.88%] [G loss: 0.826741]\n",
            "1130 [D loss: 0.565676, acc.: 71.88%] [G loss: 0.825299]\n",
            "1131 [D loss: 0.623337, acc.: 68.75%] [G loss: 0.844243]\n",
            "1132 [D loss: 0.594954, acc.: 75.00%] [G loss: 0.838847]\n",
            "1133 [D loss: 0.614309, acc.: 65.62%] [G loss: 0.846681]\n",
            "1134 [D loss: 0.623153, acc.: 62.50%] [G loss: 0.837982]\n",
            "1135 [D loss: 0.620285, acc.: 78.12%] [G loss: 0.823181]\n",
            "1136 [D loss: 0.601184, acc.: 71.88%] [G loss: 0.845444]\n",
            "1137 [D loss: 0.633482, acc.: 65.62%] [G loss: 0.847868]\n",
            "1138 [D loss: 0.591173, acc.: 81.25%] [G loss: 0.845793]\n",
            "1139 [D loss: 0.588186, acc.: 81.25%] [G loss: 0.825261]\n",
            "1140 [D loss: 0.529803, acc.: 84.38%] [G loss: 0.868421]\n",
            "1141 [D loss: 0.618837, acc.: 81.25%] [G loss: 0.876577]\n",
            "1142 [D loss: 0.570569, acc.: 87.50%] [G loss: 0.818341]\n",
            "1143 [D loss: 0.539561, acc.: 84.38%] [G loss: 0.826947]\n",
            "1144 [D loss: 0.594244, acc.: 71.88%] [G loss: 0.846416]\n",
            "1145 [D loss: 0.608291, acc.: 68.75%] [G loss: 0.837212]\n",
            "1146 [D loss: 0.572029, acc.: 75.00%] [G loss: 0.881580]\n",
            "1147 [D loss: 0.593478, acc.: 71.88%] [G loss: 0.843779]\n",
            "1148 [D loss: 0.608060, acc.: 62.50%] [G loss: 0.879072]\n",
            "1149 [D loss: 0.592820, acc.: 71.88%] [G loss: 0.825943]\n",
            "1150 [D loss: 0.587129, acc.: 78.12%] [G loss: 0.804315]\n",
            "1151 [D loss: 0.594546, acc.: 65.62%] [G loss: 0.818713]\n",
            "1152 [D loss: 0.611071, acc.: 68.75%] [G loss: 0.836349]\n",
            "1153 [D loss: 0.588948, acc.: 59.38%] [G loss: 0.858563]\n",
            "1154 [D loss: 0.579135, acc.: 75.00%] [G loss: 0.924162]\n",
            "1155 [D loss: 0.651333, acc.: 59.38%] [G loss: 0.880459]\n",
            "1156 [D loss: 0.560953, acc.: 71.88%] [G loss: 0.863600]\n",
            "1157 [D loss: 0.632085, acc.: 56.25%] [G loss: 0.852189]\n",
            "1158 [D loss: 0.606923, acc.: 62.50%] [G loss: 0.864308]\n",
            "1159 [D loss: 0.573874, acc.: 84.38%] [G loss: 0.925367]\n",
            "1160 [D loss: 0.659543, acc.: 56.25%] [G loss: 0.888595]\n",
            "1161 [D loss: 0.657267, acc.: 53.12%] [G loss: 0.884503]\n",
            "1162 [D loss: 0.653762, acc.: 53.12%] [G loss: 0.876722]\n",
            "1163 [D loss: 0.602089, acc.: 59.38%] [G loss: 0.880416]\n",
            "1164 [D loss: 0.660880, acc.: 59.38%] [G loss: 0.858357]\n",
            "1165 [D loss: 0.607029, acc.: 65.62%] [G loss: 0.824979]\n",
            "1166 [D loss: 0.652487, acc.: 65.62%] [G loss: 0.905047]\n",
            "1167 [D loss: 0.593601, acc.: 71.88%] [G loss: 0.853363]\n",
            "1168 [D loss: 0.589229, acc.: 71.88%] [G loss: 0.828536]\n",
            "1169 [D loss: 0.666257, acc.: 62.50%] [G loss: 0.871590]\n",
            "1170 [D loss: 0.602140, acc.: 59.38%] [G loss: 0.841731]\n",
            "1171 [D loss: 0.593020, acc.: 71.88%] [G loss: 0.892779]\n",
            "1172 [D loss: 0.599116, acc.: 68.75%] [G loss: 0.994831]\n",
            "1173 [D loss: 0.666284, acc.: 56.25%] [G loss: 0.866374]\n",
            "1174 [D loss: 0.622040, acc.: 78.12%] [G loss: 0.820889]\n",
            "1175 [D loss: 0.584353, acc.: 75.00%] [G loss: 0.830906]\n",
            "1176 [D loss: 0.651548, acc.: 62.50%] [G loss: 0.820044]\n",
            "1177 [D loss: 0.614710, acc.: 75.00%] [G loss: 0.841582]\n",
            "1178 [D loss: 0.643830, acc.: 65.62%] [G loss: 0.823410]\n",
            "1179 [D loss: 0.646909, acc.: 62.50%] [G loss: 0.832391]\n",
            "1180 [D loss: 0.637256, acc.: 62.50%] [G loss: 0.939653]\n",
            "1181 [D loss: 0.589366, acc.: 71.88%] [G loss: 0.945847]\n",
            "1182 [D loss: 0.600546, acc.: 71.88%] [G loss: 0.858515]\n",
            "1183 [D loss: 0.634410, acc.: 71.88%] [G loss: 0.848785]\n",
            "1184 [D loss: 0.657033, acc.: 59.38%] [G loss: 0.893160]\n",
            "1185 [D loss: 0.614168, acc.: 62.50%] [G loss: 0.808856]\n",
            "1186 [D loss: 0.630084, acc.: 68.75%] [G loss: 0.871211]\n",
            "1187 [D loss: 0.559841, acc.: 78.12%] [G loss: 0.849944]\n",
            "1188 [D loss: 0.625516, acc.: 65.62%] [G loss: 0.870407]\n",
            "1189 [D loss: 0.689149, acc.: 62.50%] [G loss: 0.858204]\n",
            "1190 [D loss: 0.589704, acc.: 78.12%] [G loss: 0.804554]\n",
            "1191 [D loss: 0.674681, acc.: 56.25%] [G loss: 0.849853]\n",
            "1192 [D loss: 0.585329, acc.: 68.75%] [G loss: 0.887954]\n",
            "1193 [D loss: 0.629948, acc.: 62.50%] [G loss: 0.899717]\n",
            "1194 [D loss: 0.618498, acc.: 59.38%] [G loss: 0.801042]\n",
            "1195 [D loss: 0.736851, acc.: 50.00%] [G loss: 0.766768]\n",
            "1196 [D loss: 0.599069, acc.: 65.62%] [G loss: 0.806439]\n",
            "1197 [D loss: 0.690019, acc.: 59.38%] [G loss: 0.892334]\n",
            "1198 [D loss: 0.568501, acc.: 78.12%] [G loss: 0.942332]\n",
            "1199 [D loss: 0.768273, acc.: 40.62%] [G loss: 0.815420]\n",
            "1200 [D loss: 0.619421, acc.: 68.75%] [G loss: 0.869407]\n",
            "1201 [D loss: 0.573247, acc.: 81.25%] [G loss: 0.891139]\n",
            "1202 [D loss: 0.678487, acc.: 46.88%] [G loss: 0.825325]\n",
            "1203 [D loss: 0.563396, acc.: 78.12%] [G loss: 0.848824]\n",
            "1204 [D loss: 0.562050, acc.: 81.25%] [G loss: 0.875996]\n",
            "1205 [D loss: 0.676724, acc.: 43.75%] [G loss: 0.823245]\n",
            "1206 [D loss: 0.608050, acc.: 68.75%] [G loss: 0.878293]\n",
            "1207 [D loss: 0.610369, acc.: 75.00%] [G loss: 0.819384]\n",
            "1208 [D loss: 0.621077, acc.: 62.50%] [G loss: 0.827318]\n",
            "1209 [D loss: 0.607698, acc.: 75.00%] [G loss: 0.805505]\n",
            "1210 [D loss: 0.569572, acc.: 78.12%] [G loss: 0.813373]\n",
            "1211 [D loss: 0.681504, acc.: 65.62%] [G loss: 0.865823]\n",
            "1212 [D loss: 0.623844, acc.: 62.50%] [G loss: 0.889109]\n",
            "1213 [D loss: 0.601369, acc.: 71.88%] [G loss: 0.858005]\n",
            "1214 [D loss: 0.650770, acc.: 68.75%] [G loss: 0.860730]\n",
            "1215 [D loss: 0.633173, acc.: 62.50%] [G loss: 0.908183]\n",
            "1216 [D loss: 0.693508, acc.: 56.25%] [G loss: 0.846267]\n",
            "1217 [D loss: 0.643434, acc.: 56.25%] [G loss: 0.898897]\n",
            "1218 [D loss: 0.619992, acc.: 71.88%] [G loss: 0.886955]\n",
            "1219 [D loss: 0.549445, acc.: 84.38%] [G loss: 0.910172]\n",
            "1220 [D loss: 0.627630, acc.: 75.00%] [G loss: 0.898456]\n",
            "1221 [D loss: 0.642169, acc.: 59.38%] [G loss: 0.872064]\n",
            "1222 [D loss: 0.601986, acc.: 68.75%] [G loss: 0.877997]\n",
            "1223 [D loss: 0.604634, acc.: 71.88%] [G loss: 0.897053]\n",
            "1224 [D loss: 0.601801, acc.: 68.75%] [G loss: 0.918422]\n",
            "1225 [D loss: 0.623032, acc.: 71.88%] [G loss: 0.825952]\n",
            "1226 [D loss: 0.648726, acc.: 56.25%] [G loss: 0.828631]\n",
            "1227 [D loss: 0.574909, acc.: 75.00%] [G loss: 0.866535]\n",
            "1228 [D loss: 0.578250, acc.: 75.00%] [G loss: 0.866266]\n",
            "1229 [D loss: 0.645052, acc.: 56.25%] [G loss: 0.794082]\n",
            "1230 [D loss: 0.622481, acc.: 65.62%] [G loss: 0.786995]\n",
            "1231 [D loss: 0.596219, acc.: 75.00%] [G loss: 0.844816]\n",
            "1232 [D loss: 0.586312, acc.: 68.75%] [G loss: 0.907869]\n",
            "1233 [D loss: 0.617877, acc.: 71.88%] [G loss: 0.825348]\n",
            "1234 [D loss: 0.642796, acc.: 59.38%] [G loss: 0.875372]\n",
            "1235 [D loss: 0.573979, acc.: 71.88%] [G loss: 0.969771]\n",
            "1236 [D loss: 0.549288, acc.: 78.12%] [G loss: 0.997191]\n",
            "1237 [D loss: 0.647893, acc.: 59.38%] [G loss: 0.940975]\n",
            "1238 [D loss: 0.596201, acc.: 75.00%] [G loss: 0.969956]\n",
            "1239 [D loss: 0.669566, acc.: 62.50%] [G loss: 0.841255]\n",
            "1240 [D loss: 0.592097, acc.: 65.62%] [G loss: 0.858659]\n",
            "1241 [D loss: 0.595418, acc.: 78.12%] [G loss: 0.886777]\n",
            "1242 [D loss: 0.653854, acc.: 59.38%] [G loss: 0.925980]\n",
            "1243 [D loss: 0.658519, acc.: 62.50%] [G loss: 0.958313]\n",
            "1244 [D loss: 0.637058, acc.: 59.38%] [G loss: 0.911958]\n",
            "1245 [D loss: 0.620507, acc.: 65.62%] [G loss: 0.878932]\n",
            "1246 [D loss: 0.603856, acc.: 68.75%] [G loss: 0.882989]\n",
            "1247 [D loss: 0.601313, acc.: 62.50%] [G loss: 0.882621]\n",
            "1248 [D loss: 0.617755, acc.: 59.38%] [G loss: 0.910841]\n",
            "1249 [D loss: 0.567600, acc.: 81.25%] [G loss: 0.943962]\n",
            "1250 [D loss: 0.622403, acc.: 59.38%] [G loss: 1.010725]\n",
            "1251 [D loss: 0.597661, acc.: 75.00%] [G loss: 0.976680]\n",
            "1252 [D loss: 0.646080, acc.: 59.38%] [G loss: 0.906578]\n",
            "1253 [D loss: 0.611758, acc.: 68.75%] [G loss: 0.917576]\n",
            "1254 [D loss: 0.652863, acc.: 59.38%] [G loss: 0.876221]\n",
            "1255 [D loss: 0.648692, acc.: 59.38%] [G loss: 0.803967]\n",
            "1256 [D loss: 0.669448, acc.: 59.38%] [G loss: 0.753976]\n",
            "1257 [D loss: 0.622703, acc.: 59.38%] [G loss: 0.852076]\n",
            "1258 [D loss: 0.610485, acc.: 65.62%] [G loss: 0.890447]\n",
            "1259 [D loss: 0.585694, acc.: 81.25%] [G loss: 0.911918]\n",
            "1260 [D loss: 0.569796, acc.: 81.25%] [G loss: 0.887372]\n",
            "1261 [D loss: 0.615117, acc.: 59.38%] [G loss: 0.930960]\n",
            "1262 [D loss: 0.642749, acc.: 68.75%] [G loss: 0.923341]\n",
            "1263 [D loss: 0.531486, acc.: 84.38%] [G loss: 0.985164]\n",
            "1264 [D loss: 0.615335, acc.: 75.00%] [G loss: 0.887480]\n",
            "1265 [D loss: 0.647606, acc.: 62.50%] [G loss: 0.911218]\n",
            "1266 [D loss: 0.616395, acc.: 62.50%] [G loss: 0.894883]\n",
            "1267 [D loss: 0.593581, acc.: 81.25%] [G loss: 0.843598]\n",
            "1268 [D loss: 0.599627, acc.: 71.88%] [G loss: 0.849473]\n",
            "1269 [D loss: 0.663276, acc.: 65.62%] [G loss: 0.881310]\n",
            "1270 [D loss: 0.561992, acc.: 81.25%] [G loss: 0.904076]\n",
            "1271 [D loss: 0.594287, acc.: 65.62%] [G loss: 0.882764]\n",
            "1272 [D loss: 0.647419, acc.: 62.50%] [G loss: 0.884587]\n",
            "1273 [D loss: 0.654481, acc.: 62.50%] [G loss: 0.878036]\n",
            "1274 [D loss: 0.657568, acc.: 56.25%] [G loss: 0.818862]\n",
            "1275 [D loss: 0.652270, acc.: 59.38%] [G loss: 0.861139]\n",
            "1276 [D loss: 0.632637, acc.: 56.25%] [G loss: 1.030353]\n",
            "1277 [D loss: 0.650289, acc.: 65.62%] [G loss: 0.987687]\n",
            "1278 [D loss: 0.701457, acc.: 43.75%] [G loss: 0.953552]\n",
            "1279 [D loss: 0.632914, acc.: 62.50%] [G loss: 0.925961]\n",
            "1280 [D loss: 0.636876, acc.: 62.50%] [G loss: 0.852013]\n",
            "1281 [D loss: 0.592219, acc.: 75.00%] [G loss: 0.821724]\n",
            "1282 [D loss: 0.687681, acc.: 43.75%] [G loss: 0.849753]\n",
            "1283 [D loss: 0.626764, acc.: 68.75%] [G loss: 0.895697]\n",
            "1284 [D loss: 0.560031, acc.: 84.38%] [G loss: 0.923574]\n",
            "1285 [D loss: 0.638447, acc.: 59.38%] [G loss: 0.971470]\n",
            "1286 [D loss: 0.588611, acc.: 75.00%] [G loss: 1.046902]\n",
            "1287 [D loss: 0.663021, acc.: 68.75%] [G loss: 0.858290]\n",
            "1288 [D loss: 0.599538, acc.: 59.38%] [G loss: 0.874453]\n",
            "1289 [D loss: 0.595592, acc.: 78.12%] [G loss: 0.864818]\n",
            "1290 [D loss: 0.634590, acc.: 65.62%] [G loss: 0.833933]\n",
            "1291 [D loss: 0.633966, acc.: 59.38%] [G loss: 0.841097]\n",
            "1292 [D loss: 0.657114, acc.: 53.12%] [G loss: 0.801091]\n",
            "1293 [D loss: 0.640630, acc.: 65.62%] [G loss: 0.826795]\n",
            "1294 [D loss: 0.550714, acc.: 78.12%] [G loss: 0.867580]\n",
            "1295 [D loss: 0.682132, acc.: 53.12%] [G loss: 0.891010]\n",
            "1296 [D loss: 0.603553, acc.: 71.88%] [G loss: 0.845587]\n",
            "1297 [D loss: 0.540205, acc.: 78.12%] [G loss: 0.871133]\n",
            "1298 [D loss: 0.639264, acc.: 59.38%] [G loss: 0.875387]\n",
            "1299 [D loss: 0.631672, acc.: 59.38%] [G loss: 0.819008]\n",
            "1300 [D loss: 0.539593, acc.: 81.25%] [G loss: 0.800939]\n",
            "1301 [D loss: 0.531168, acc.: 84.38%] [G loss: 0.817714]\n",
            "1302 [D loss: 0.569411, acc.: 78.12%] [G loss: 0.818756]\n",
            "1303 [D loss: 0.604537, acc.: 62.50%] [G loss: 0.914612]\n",
            "1304 [D loss: 0.648389, acc.: 65.62%] [G loss: 0.920193]\n",
            "1305 [D loss: 0.599169, acc.: 68.75%] [G loss: 0.849464]\n",
            "1306 [D loss: 0.644982, acc.: 71.88%] [G loss: 0.860277]\n",
            "1307 [D loss: 0.533829, acc.: 84.38%] [G loss: 0.827667]\n",
            "1308 [D loss: 0.644128, acc.: 68.75%] [G loss: 0.874866]\n",
            "1309 [D loss: 0.551924, acc.: 68.75%] [G loss: 0.922444]\n",
            "1310 [D loss: 0.661062, acc.: 65.62%] [G loss: 0.861163]\n",
            "1311 [D loss: 0.547808, acc.: 81.25%] [G loss: 0.850668]\n",
            "1312 [D loss: 0.556882, acc.: 68.75%] [G loss: 0.882849]\n",
            "1313 [D loss: 0.609122, acc.: 65.62%] [G loss: 0.920431]\n",
            "1314 [D loss: 0.556444, acc.: 84.38%] [G loss: 0.930779]\n",
            "1315 [D loss: 0.609694, acc.: 78.12%] [G loss: 0.892778]\n",
            "1316 [D loss: 0.587579, acc.: 84.38%] [G loss: 0.846198]\n",
            "1317 [D loss: 0.626769, acc.: 68.75%] [G loss: 0.859941]\n",
            "1318 [D loss: 0.615523, acc.: 65.62%] [G loss: 0.861192]\n",
            "1319 [D loss: 0.661634, acc.: 50.00%] [G loss: 0.877159]\n",
            "1320 [D loss: 0.610479, acc.: 62.50%] [G loss: 0.867437]\n",
            "1321 [D loss: 0.558409, acc.: 81.25%] [G loss: 0.874751]\n",
            "1322 [D loss: 0.585795, acc.: 81.25%] [G loss: 0.873034]\n",
            "1323 [D loss: 0.710449, acc.: 50.00%] [G loss: 0.871014]\n",
            "1324 [D loss: 0.596014, acc.: 71.88%] [G loss: 0.875737]\n",
            "1325 [D loss: 0.615747, acc.: 68.75%] [G loss: 0.843377]\n",
            "1326 [D loss: 0.604454, acc.: 65.62%] [G loss: 0.881229]\n",
            "1327 [D loss: 0.606311, acc.: 65.62%] [G loss: 0.916240]\n",
            "1328 [D loss: 0.634129, acc.: 71.88%] [G loss: 0.915013]\n",
            "1329 [D loss: 0.589185, acc.: 68.75%] [G loss: 0.945014]\n",
            "1330 [D loss: 0.614577, acc.: 68.75%] [G loss: 0.936244]\n",
            "1331 [D loss: 0.605298, acc.: 71.88%] [G loss: 0.878246]\n",
            "1332 [D loss: 0.629218, acc.: 65.62%] [G loss: 0.866287]\n",
            "1333 [D loss: 0.658088, acc.: 53.12%] [G loss: 0.873149]\n",
            "1334 [D loss: 0.602727, acc.: 68.75%] [G loss: 0.852017]\n",
            "1335 [D loss: 0.612513, acc.: 62.50%] [G loss: 0.866062]\n",
            "1336 [D loss: 0.653882, acc.: 62.50%] [G loss: 0.918175]\n",
            "1337 [D loss: 0.541478, acc.: 78.12%] [G loss: 0.907425]\n",
            "1338 [D loss: 0.632321, acc.: 62.50%] [G loss: 0.918989]\n",
            "1339 [D loss: 0.585793, acc.: 68.75%] [G loss: 0.906401]\n",
            "1340 [D loss: 0.631303, acc.: 65.62%] [G loss: 0.877506]\n",
            "1341 [D loss: 0.603717, acc.: 65.62%] [G loss: 0.873081]\n",
            "1342 [D loss: 0.520755, acc.: 78.12%] [G loss: 0.936090]\n",
            "1343 [D loss: 0.651275, acc.: 59.38%] [G loss: 0.869070]\n",
            "1344 [D loss: 0.592184, acc.: 71.88%] [G loss: 0.939036]\n",
            "1345 [D loss: 0.616835, acc.: 59.38%] [G loss: 0.884894]\n",
            "1346 [D loss: 0.598551, acc.: 65.62%] [G loss: 0.898007]\n",
            "1347 [D loss: 0.657577, acc.: 53.12%] [G loss: 0.954540]\n",
            "1348 [D loss: 0.552408, acc.: 78.12%] [G loss: 0.867159]\n",
            "1349 [D loss: 0.636489, acc.: 62.50%] [G loss: 0.888631]\n",
            "1350 [D loss: 0.601355, acc.: 71.88%] [G loss: 0.943235]\n",
            "1351 [D loss: 0.699431, acc.: 53.12%] [G loss: 0.930537]\n",
            "1352 [D loss: 0.597198, acc.: 62.50%] [G loss: 0.966536]\n",
            "1353 [D loss: 0.582217, acc.: 78.12%] [G loss: 1.033766]\n",
            "1354 [D loss: 0.601207, acc.: 71.88%] [G loss: 0.975335]\n",
            "1355 [D loss: 0.643052, acc.: 56.25%] [G loss: 0.915625]\n",
            "1356 [D loss: 0.664778, acc.: 59.38%] [G loss: 0.977876]\n",
            "1357 [D loss: 0.668479, acc.: 56.25%] [G loss: 0.875871]\n",
            "1358 [D loss: 0.681923, acc.: 65.62%] [G loss: 0.817930]\n",
            "1359 [D loss: 0.598671, acc.: 65.62%] [G loss: 0.863153]\n",
            "1360 [D loss: 0.564592, acc.: 71.88%] [G loss: 0.913753]\n",
            "1361 [D loss: 0.602246, acc.: 75.00%] [G loss: 0.910616]\n",
            "1362 [D loss: 0.497505, acc.: 87.50%] [G loss: 0.893923]\n",
            "1363 [D loss: 0.610981, acc.: 75.00%] [G loss: 0.875221]\n",
            "1364 [D loss: 0.620366, acc.: 56.25%] [G loss: 0.871766]\n",
            "1365 [D loss: 0.630181, acc.: 65.62%] [G loss: 0.815603]\n",
            "1366 [D loss: 0.570223, acc.: 75.00%] [G loss: 0.827898]\n",
            "1367 [D loss: 0.688319, acc.: 59.38%] [G loss: 0.809223]\n",
            "1368 [D loss: 0.650750, acc.: 62.50%] [G loss: 0.769670]\n",
            "1369 [D loss: 0.658091, acc.: 59.38%] [G loss: 0.793555]\n",
            "1370 [D loss: 0.569515, acc.: 75.00%] [G loss: 0.807843]\n",
            "1371 [D loss: 0.576863, acc.: 71.88%] [G loss: 0.914369]\n",
            "1372 [D loss: 0.618656, acc.: 71.88%] [G loss: 0.938075]\n",
            "1373 [D loss: 0.629077, acc.: 65.62%] [G loss: 0.951764]\n",
            "1374 [D loss: 0.565567, acc.: 78.12%] [G loss: 0.920684]\n",
            "1375 [D loss: 0.595112, acc.: 65.62%] [G loss: 0.835326]\n",
            "1376 [D loss: 0.664891, acc.: 59.38%] [G loss: 0.889442]\n",
            "1377 [D loss: 0.655724, acc.: 56.25%] [G loss: 0.903262]\n",
            "1378 [D loss: 0.579904, acc.: 75.00%] [G loss: 0.863548]\n",
            "1379 [D loss: 0.552432, acc.: 65.62%] [G loss: 1.008131]\n",
            "1380 [D loss: 0.605993, acc.: 65.62%] [G loss: 0.982479]\n",
            "1381 [D loss: 0.653054, acc.: 65.62%] [G loss: 0.917282]\n",
            "1382 [D loss: 0.617511, acc.: 59.38%] [G loss: 0.903882]\n",
            "1383 [D loss: 0.620062, acc.: 53.12%] [G loss: 0.854599]\n",
            "1384 [D loss: 0.529399, acc.: 81.25%] [G loss: 0.848990]\n",
            "1385 [D loss: 0.649360, acc.: 59.38%] [G loss: 0.908899]\n",
            "1386 [D loss: 0.599869, acc.: 75.00%] [G loss: 0.825151]\n",
            "1387 [D loss: 0.604490, acc.: 68.75%] [G loss: 0.833992]\n",
            "1388 [D loss: 0.610999, acc.: 56.25%] [G loss: 0.846309]\n",
            "1389 [D loss: 0.670819, acc.: 53.12%] [G loss: 0.852147]\n",
            "1390 [D loss: 0.590550, acc.: 65.62%] [G loss: 0.882869]\n",
            "1391 [D loss: 0.571338, acc.: 75.00%] [G loss: 0.920294]\n",
            "1392 [D loss: 0.607437, acc.: 68.75%] [G loss: 0.907685]\n",
            "1393 [D loss: 0.627096, acc.: 71.88%] [G loss: 0.927106]\n",
            "1394 [D loss: 0.584743, acc.: 78.12%] [G loss: 0.859897]\n",
            "1395 [D loss: 0.577535, acc.: 78.12%] [G loss: 0.869807]\n",
            "1396 [D loss: 0.642336, acc.: 65.62%] [G loss: 0.879269]\n",
            "1397 [D loss: 0.590432, acc.: 71.88%] [G loss: 0.843852]\n",
            "1398 [D loss: 0.613324, acc.: 75.00%] [G loss: 0.810659]\n",
            "1399 [D loss: 0.574675, acc.: 68.75%] [G loss: 0.800603]\n",
            "1400 [D loss: 0.523765, acc.: 78.12%] [G loss: 0.879453]\n",
            "1401 [D loss: 0.551141, acc.: 75.00%] [G loss: 0.927556]\n",
            "1402 [D loss: 0.553478, acc.: 71.88%] [G loss: 0.931811]\n",
            "1403 [D loss: 0.648133, acc.: 56.25%] [G loss: 0.865361]\n",
            "1404 [D loss: 0.647336, acc.: 59.38%] [G loss: 0.880111]\n",
            "1405 [D loss: 0.664561, acc.: 62.50%] [G loss: 0.882457]\n",
            "1406 [D loss: 0.602816, acc.: 62.50%] [G loss: 0.927506]\n",
            "1407 [D loss: 0.572270, acc.: 68.75%] [G loss: 0.855616]\n",
            "1408 [D loss: 0.674752, acc.: 59.38%] [G loss: 0.873806]\n",
            "1409 [D loss: 0.632519, acc.: 62.50%] [G loss: 0.927214]\n",
            "1410 [D loss: 0.601081, acc.: 71.88%] [G loss: 0.911308]\n",
            "1411 [D loss: 0.608771, acc.: 62.50%] [G loss: 0.919261]\n",
            "1412 [D loss: 0.531421, acc.: 81.25%] [G loss: 0.872373]\n",
            "1413 [D loss: 0.605680, acc.: 71.88%] [G loss: 0.945512]\n",
            "1414 [D loss: 0.521411, acc.: 68.75%] [G loss: 0.945097]\n",
            "1415 [D loss: 0.543138, acc.: 81.25%] [G loss: 0.861463]\n",
            "1416 [D loss: 0.588259, acc.: 62.50%] [G loss: 0.880370]\n",
            "1417 [D loss: 0.612810, acc.: 59.38%] [G loss: 0.953891]\n",
            "1418 [D loss: 0.598665, acc.: 68.75%] [G loss: 0.908817]\n",
            "1419 [D loss: 0.580918, acc.: 71.88%] [G loss: 0.919548]\n",
            "1420 [D loss: 0.617848, acc.: 75.00%] [G loss: 0.887232]\n",
            "1421 [D loss: 0.632199, acc.: 71.88%] [G loss: 0.985721]\n",
            "1422 [D loss: 0.634434, acc.: 56.25%] [G loss: 0.983502]\n",
            "1423 [D loss: 0.603499, acc.: 78.12%] [G loss: 0.866585]\n",
            "1424 [D loss: 0.548268, acc.: 84.38%] [G loss: 0.876449]\n",
            "1425 [D loss: 0.645294, acc.: 62.50%] [G loss: 0.912266]\n",
            "1426 [D loss: 0.507999, acc.: 78.12%] [G loss: 0.888245]\n",
            "1427 [D loss: 0.625731, acc.: 75.00%] [G loss: 0.840199]\n",
            "1428 [D loss: 0.630220, acc.: 62.50%] [G loss: 0.903976]\n",
            "1429 [D loss: 0.636741, acc.: 65.62%] [G loss: 0.927492]\n",
            "1430 [D loss: 0.660175, acc.: 59.38%] [G loss: 0.830933]\n",
            "1431 [D loss: 0.525168, acc.: 81.25%] [G loss: 0.883935]\n",
            "1432 [D loss: 0.619922, acc.: 62.50%] [G loss: 0.827184]\n",
            "1433 [D loss: 0.610944, acc.: 75.00%] [G loss: 0.851876]\n",
            "1434 [D loss: 0.570016, acc.: 68.75%] [G loss: 0.843659]\n",
            "1435 [D loss: 0.524048, acc.: 84.38%] [G loss: 0.956884]\n",
            "1436 [D loss: 0.608519, acc.: 50.00%] [G loss: 1.006816]\n",
            "1437 [D loss: 0.605977, acc.: 75.00%] [G loss: 0.986236]\n",
            "1438 [D loss: 0.596928, acc.: 75.00%] [G loss: 0.968828]\n",
            "1439 [D loss: 0.637710, acc.: 68.75%] [G loss: 0.880297]\n",
            "1440 [D loss: 0.631540, acc.: 59.38%] [G loss: 0.936850]\n",
            "1441 [D loss: 0.601126, acc.: 71.88%] [G loss: 0.946057]\n",
            "1442 [D loss: 0.665546, acc.: 62.50%] [G loss: 0.892005]\n",
            "1443 [D loss: 0.667939, acc.: 62.50%] [G loss: 0.921419]\n",
            "1444 [D loss: 0.680210, acc.: 56.25%] [G loss: 0.866410]\n",
            "1445 [D loss: 0.560178, acc.: 81.25%] [G loss: 0.964712]\n",
            "1446 [D loss: 0.599643, acc.: 71.88%] [G loss: 0.880097]\n",
            "1447 [D loss: 0.546732, acc.: 78.12%] [G loss: 0.898675]\n",
            "1448 [D loss: 0.632088, acc.: 65.62%] [G loss: 0.823916]\n",
            "1449 [D loss: 0.661895, acc.: 56.25%] [G loss: 0.788086]\n",
            "1450 [D loss: 0.519272, acc.: 81.25%] [G loss: 0.826391]\n",
            "1451 [D loss: 0.599618, acc.: 71.88%] [G loss: 0.849631]\n",
            "1452 [D loss: 0.625068, acc.: 71.88%] [G loss: 0.837807]\n",
            "1453 [D loss: 0.589440, acc.: 68.75%] [G loss: 0.874508]\n",
            "1454 [D loss: 0.609194, acc.: 62.50%] [G loss: 0.833683]\n",
            "1455 [D loss: 0.622889, acc.: 65.62%] [G loss: 0.857462]\n",
            "1456 [D loss: 0.724156, acc.: 43.75%] [G loss: 0.896574]\n",
            "1457 [D loss: 0.615908, acc.: 75.00%] [G loss: 0.945857]\n",
            "1458 [D loss: 0.653723, acc.: 50.00%] [G loss: 1.025263]\n",
            "1459 [D loss: 0.632042, acc.: 68.75%] [G loss: 0.913168]\n",
            "1460 [D loss: 0.612954, acc.: 71.88%] [G loss: 0.952565]\n",
            "1461 [D loss: 0.629346, acc.: 65.62%] [G loss: 0.935194]\n",
            "1462 [D loss: 0.607400, acc.: 68.75%] [G loss: 0.909485]\n",
            "1463 [D loss: 0.626091, acc.: 53.12%] [G loss: 0.905631]\n",
            "1464 [D loss: 0.649559, acc.: 59.38%] [G loss: 0.871592]\n",
            "1465 [D loss: 0.602249, acc.: 71.88%] [G loss: 0.886718]\n",
            "1466 [D loss: 0.566422, acc.: 68.75%] [G loss: 0.864401]\n",
            "1467 [D loss: 0.589069, acc.: 65.62%] [G loss: 0.884207]\n",
            "1468 [D loss: 0.585455, acc.: 75.00%] [G loss: 0.906040]\n",
            "1469 [D loss: 0.592969, acc.: 68.75%] [G loss: 0.940855]\n",
            "1470 [D loss: 0.668493, acc.: 50.00%] [G loss: 0.852838]\n",
            "1471 [D loss: 0.660788, acc.: 53.12%] [G loss: 0.869697]\n",
            "1472 [D loss: 0.710662, acc.: 40.62%] [G loss: 0.890931]\n",
            "1473 [D loss: 0.535632, acc.: 81.25%] [G loss: 0.957682]\n",
            "1474 [D loss: 0.554307, acc.: 81.25%] [G loss: 1.017484]\n",
            "1475 [D loss: 0.590379, acc.: 71.88%] [G loss: 0.966215]\n",
            "1476 [D loss: 0.539868, acc.: 93.75%] [G loss: 0.966190]\n",
            "1477 [D loss: 0.647864, acc.: 62.50%] [G loss: 0.880410]\n",
            "1478 [D loss: 0.546189, acc.: 68.75%] [G loss: 0.906121]\n",
            "1479 [D loss: 0.568167, acc.: 78.12%] [G loss: 0.929399]\n",
            "1480 [D loss: 0.592116, acc.: 71.88%] [G loss: 0.839567]\n",
            "1481 [D loss: 0.603301, acc.: 65.62%] [G loss: 0.899957]\n",
            "1482 [D loss: 0.580872, acc.: 75.00%] [G loss: 0.910942]\n",
            "1483 [D loss: 0.547227, acc.: 81.25%] [G loss: 0.981160]\n",
            "1484 [D loss: 0.603423, acc.: 78.12%] [G loss: 0.921401]\n",
            "1485 [D loss: 0.556621, acc.: 84.38%] [G loss: 0.941862]\n",
            "1486 [D loss: 0.537951, acc.: 71.88%] [G loss: 0.996405]\n",
            "1487 [D loss: 0.664467, acc.: 56.25%] [G loss: 0.945966]\n",
            "1488 [D loss: 0.582411, acc.: 71.88%] [G loss: 0.892763]\n",
            "1489 [D loss: 0.571410, acc.: 75.00%] [G loss: 0.935078]\n",
            "1490 [D loss: 0.568408, acc.: 62.50%] [G loss: 0.976386]\n",
            "1491 [D loss: 0.613667, acc.: 71.88%] [G loss: 0.966658]\n",
            "1492 [D loss: 0.521237, acc.: 84.38%] [G loss: 0.930456]\n",
            "1493 [D loss: 0.628152, acc.: 59.38%] [G loss: 0.840026]\n",
            "1494 [D loss: 0.612069, acc.: 59.38%] [G loss: 0.916083]\n",
            "1495 [D loss: 0.631215, acc.: 46.88%] [G loss: 0.947190]\n",
            "1496 [D loss: 0.578134, acc.: 71.88%] [G loss: 0.991133]\n",
            "1497 [D loss: 0.579698, acc.: 62.50%] [G loss: 1.028760]\n",
            "1498 [D loss: 0.606519, acc.: 68.75%] [G loss: 0.929000]\n",
            "1499 [D loss: 0.616376, acc.: 71.88%] [G loss: 0.917290]\n",
            "1500 [D loss: 0.588505, acc.: 75.00%] [G loss: 0.907801]\n",
            "1501 [D loss: 0.595133, acc.: 65.62%] [G loss: 0.863545]\n",
            "1502 [D loss: 0.616796, acc.: 65.62%] [G loss: 0.933602]\n",
            "1503 [D loss: 0.479338, acc.: 93.75%] [G loss: 0.942728]\n",
            "1504 [D loss: 0.616481, acc.: 65.62%] [G loss: 0.904782]\n",
            "1505 [D loss: 0.522078, acc.: 81.25%] [G loss: 1.007007]\n",
            "1506 [D loss: 0.673014, acc.: 53.12%] [G loss: 0.924447]\n",
            "1507 [D loss: 0.623052, acc.: 71.88%] [G loss: 0.816431]\n",
            "1508 [D loss: 0.571082, acc.: 75.00%] [G loss: 0.872717]\n",
            "1509 [D loss: 0.574535, acc.: 75.00%] [G loss: 0.829269]\n",
            "1510 [D loss: 0.580133, acc.: 65.62%] [G loss: 0.887026]\n",
            "1511 [D loss: 0.566091, acc.: 75.00%] [G loss: 0.838406]\n",
            "1512 [D loss: 0.619397, acc.: 62.50%] [G loss: 0.879411]\n",
            "1513 [D loss: 0.549993, acc.: 81.25%] [G loss: 0.838071]\n",
            "1514 [D loss: 0.653930, acc.: 71.88%] [G loss: 0.882863]\n",
            "1515 [D loss: 0.601953, acc.: 75.00%] [G loss: 0.890961]\n",
            "1516 [D loss: 0.625076, acc.: 68.75%] [G loss: 0.954907]\n",
            "1517 [D loss: 0.583470, acc.: 78.12%] [G loss: 0.916970]\n",
            "1518 [D loss: 0.665590, acc.: 59.38%] [G loss: 0.902342]\n",
            "1519 [D loss: 0.686469, acc.: 46.88%] [G loss: 0.961147]\n",
            "1520 [D loss: 0.650178, acc.: 53.12%] [G loss: 0.916996]\n",
            "1521 [D loss: 0.644287, acc.: 75.00%] [G loss: 0.924388]\n",
            "1522 [D loss: 0.596172, acc.: 65.62%] [G loss: 0.922413]\n",
            "1523 [D loss: 0.580106, acc.: 78.12%] [G loss: 0.913873]\n",
            "1524 [D loss: 0.647643, acc.: 62.50%] [G loss: 0.946502]\n",
            "1525 [D loss: 0.587272, acc.: 65.62%] [G loss: 0.988485]\n",
            "1526 [D loss: 0.495128, acc.: 90.62%] [G loss: 1.018795]\n",
            "1527 [D loss: 0.661124, acc.: 68.75%] [G loss: 0.903733]\n",
            "1528 [D loss: 0.583537, acc.: 68.75%] [G loss: 0.889522]\n",
            "1529 [D loss: 0.571860, acc.: 78.12%] [G loss: 0.847580]\n",
            "1530 [D loss: 0.551079, acc.: 68.75%] [G loss: 0.956287]\n",
            "1531 [D loss: 0.604549, acc.: 68.75%] [G loss: 0.948097]\n",
            "1532 [D loss: 0.590550, acc.: 71.88%] [G loss: 0.877486]\n",
            "1533 [D loss: 0.686393, acc.: 53.12%] [G loss: 0.914901]\n",
            "1534 [D loss: 0.563150, acc.: 68.75%] [G loss: 0.955154]\n",
            "1535 [D loss: 0.601588, acc.: 71.88%] [G loss: 0.954716]\n",
            "1536 [D loss: 0.631509, acc.: 75.00%] [G loss: 0.856557]\n",
            "1537 [D loss: 0.622876, acc.: 65.62%] [G loss: 0.848060]\n",
            "1538 [D loss: 0.649352, acc.: 71.88%] [G loss: 0.911451]\n",
            "1539 [D loss: 0.614463, acc.: 71.88%] [G loss: 0.826417]\n",
            "1540 [D loss: 0.615848, acc.: 62.50%] [G loss: 0.875520]\n",
            "1541 [D loss: 0.540402, acc.: 81.25%] [G loss: 0.931944]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-4cb385fe73b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-608535804c27>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "-sEl0-rS-Dqq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Modified GAN for CIFAR10"
      ]
    },
    {
      "metadata": {
        "id": "3niubA2q4KBD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oZCAsYGs-Cq4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GAN():\n",
        "  def __init__(self):\n",
        "    self.img_rows = 32\n",
        "    self.img_cols = 32\n",
        "    self.channels = 3\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    optimizer = Adam(0.0002, 0.5)\n",
        "    \n",
        "    # Discriminator\n",
        "    self.discriminator = self.build_discriminator()\n",
        "    self.discriminator.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    # Generator\n",
        "    self.generator = self.build_generator()\n",
        "    self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "    z = Input(shape=(900,))\n",
        "    img = self.generator(z)\n",
        "    \n",
        "    self.discriminator.trainable = False\n",
        "    \n",
        "    valid = self.discriminator(img)\n",
        "    \n",
        "    self.combined = Model(z, valid)\n",
        "    self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "  def build_generator(self):\n",
        "    noise_shape = (900,)\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_shape = noise_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(2048))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "    model.add(Reshape(self.img_shape))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    noise = Input(shape=noise_shape)\n",
        "    img = model(noise)\n",
        "    \n",
        "    return Model(noise,img)\n",
        "    \n",
        "  def build_discriminator(self):\n",
        "    img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=img_shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(32, (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    \n",
        "    img = Input(shape=img_shape)\n",
        "    validity = model(img)\n",
        "    \n",
        "    return Model(img, validity)\n",
        "  \n",
        "  def train(self, epochs, batch_size=128, save_interval=50):\n",
        "    (X_train, _), (_,_) = cifar10.load_data()\n",
        "    \n",
        "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
        "    #X_train = np.expand_dims(X_train, axis=3)\n",
        "    \n",
        "    half_batch = int(batch_size / 2)\n",
        "    # Adversarial ground truths\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      # Discriminator\n",
        "      idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "      imgs = X_train[idx]\n",
        "      \n",
        "      noise = np.random.normal(0, 1, (batch_size, 900))\n",
        "      \n",
        "      gen_imgs = self.generator.predict(noise)\n",
        "      \n",
        "      d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "      d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "      \n",
        "      # Generator\n",
        "      noise = np.random.normal(0, 1, (batch_size, 900))\n",
        "      valid_y = np.array([1] * batch_size)\n",
        "      \n",
        "      g_loss = self.combined.train_on_batch(noise, valid_y)\n",
        "      \n",
        "      # plot progress\n",
        "      #print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "      \n",
        "      # If at save interval => save generated image samples\n",
        "      if epoch % save_interval == 0:\n",
        "         self.save_imgs(epoch)\n",
        "          \n",
        "  def save_imgs(self, epoch):\n",
        "      r, c = 5, 5\n",
        "      noise = np.random.normal(0, 1, (r * c, 900))\n",
        "      gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "      # Rescale images 0 - 1\n",
        "      gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "      fig, axs = plt.subplots(r, c)\n",
        "      cnt = 0\n",
        "      for i in range(r):\n",
        "          for j in range(c):\n",
        "              axs[i,j].imshow(gen_imgs[cnt])\n",
        "              axs[i,j].axis('off')\n",
        "              cnt += 1\n",
        "      fig.savefig(\"cifar10_%d.png\" % epoch)\n",
        "      files.download(\"cifar10_%d.png\" % epoch)\n",
        "      plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UFYAWWj2-mIM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1601
        },
        "outputId": "0e1ffd20-3b0b-4cb3-9201-25963812fc13"
      },
      "cell_type": "code",
      "source": [
        "gan = GAN()\n",
        "gan.train(epochs=30000, batch_size=32, save_interval=200)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_29 (Flatten)         (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_211 (Dense)            (None, 1024)              2360320   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_155 (LeakyReLU)  (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_212 (Dense)            (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_156 (LeakyReLU)  (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_213 (Dense)            (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_157 (LeakyReLU)  (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_214 (Dense)            (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 3,082,273\n",
            "Trainable params: 3,082,273\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_215 (Dense)            (None, 256)               230656    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_158 (LeakyReLU)  (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_86 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_216 (Dense)            (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_159 (LeakyReLU)  (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_217 (Dense)            (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_160 (LeakyReLU)  (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_218 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_161 (LeakyReLU)  (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_219 (Dense)            (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_162 (LeakyReLU)  (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_88 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_220 (Dense)            (None, 2048)              2099200   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_163 (LeakyReLU)  (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_89 (Batc (None, 2048)              8192      \n",
            "_________________________________________________________________\n",
            "dense_221 (Dense)            (None, 3072)              6294528   \n",
            "_________________________________________________________________\n",
            "reshape_28 (Reshape)         (None, 32, 32, 3)         0         \n",
            "=================================================================\n",
            "Total params: 11,395,840\n",
            "Trainable params: 11,388,160\n",
            "Non-trainable params: 7,680\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "JyiVXSX5bPWS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**INSTALL MAGENTA**"
      ]
    },
    {
      "metadata": {
        "id": "bMnDALPcTPLg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@test {\"output\": \"ignore\"}\n",
        "\n",
        "print('Installing dependencies...')\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -qU pyfluidsynth pretty_midi\n",
        "\n",
        "# Temporary hack since the colab installs a RC version of tensorflow.\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install -q magenta-gpu\n",
        "\n",
        "# Hack to allow python to pick up the newly-installed fluidsynth lib. \n",
        "# This is only needed for the hosted Colab environment.\n",
        "import ctypes.util\n",
        "orig_ctypes_util_find_library = ctypes.util.find_library\n",
        "def proxy_find_library(lib):\n",
        "  if lib == 'fluidsynth':\n",
        "    return 'libfluidsynth.so.1'\n",
        "  else:\n",
        "    return orig_ctypes_util_find_library(lib)\n",
        "ctypes.util.find_library = proxy_find_library\n",
        "\n",
        "print('Importing libraries and defining some helper functions...')\n",
        "from google.colab import files\n",
        "\n",
        "import magenta.music as mm\n",
        "import magenta\n",
        "import tensorflow\n",
        "\n",
        "print(' Done!')\n",
        "print(magenta.__version__ )\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OnYo5_Pz-UYC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}